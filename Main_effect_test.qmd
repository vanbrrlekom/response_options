---
title: "response options"
format: html
editor: visual
---

#### Recap

I'm going to quote myself from the previous notebook that I wrote, to describe the main thrust of the experiment. 

>"So for the experiment, we produced morphed faces of different levels of femininity and masculinity. There were 18 continua, where gender varied in seven increments, for a total of 126 faces.

> There were five response options conditions:

> 1.  binary categories - man/woman
> 2.  multiple categories - man/woman/other/don't know
> 3.  Freetext - a free text box
> 4.  binary dimension - woman ------- man on a slider
> 5.  multiple dimensions - woman / man on separate sliders.

At the previous meeting, we decided that i would look at answering the auestion "do people use non-binary options when they have them?"

The way I've gone about doing that is by primarily comparing option 2 and option 3 from above.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#Start by loading dependencies and the data

library(brms)
library(tidyr)
library(dplyr)
library(bayesplot)
library(tidybayes)
library(ggplot2)
source("src/functions.r")
d  <- read_and_clean("data/cat2stduy1_data.csv") %>% 
  mutate(fem = 100-masc) 
```

### Do people use the beyond-binary options?

#### Main effects

The way I went about testing that is by comparing the amount of beyond-binary (i.e. "other" and "I don't know" responses) across the free text and multiple categories conditions. Here's snippet of the data for some context:

```{r message=FALSE, warning=FALSE}
#Well, fist, first I wrangle data
tmp <- d %>% 
  filter(condition == "mc" | condition == "ft") %>% 
  mutate(categorization = recode(categorization, 
                                 "F" = "f", "kvinna" = "f", "female" = "f", "female " = "f", "Female"= "f", "Fenale"= "f", "women" = "f", "woman " = "f", "femLE" = "f", "FEmale" = "f", "Femalw" = "f", "Fwmalw" = "f", "Female " = "f", "woman" = "f", "Woman" = "f", "feMale" = "f", "fermale" = "f", "wman" = "f", "Femae" = "f",
                                 "man" = "m","Male " = "m", "make" = "m", "Male"= "m", "male" = "m", "man " = "m",  "male " = "m", "guy" = "m", "boy" = "m", "Make" = "m", "M"  = "m", "Man" = "m", "Bottom half male; above nose female., Would have to say Male" = "m", " male" = "m", "male  " = "m", "ale" = "m", "nmale" = "m", "MALE"= "m", "nale"= "m", " Male" = "m",
                                 "Nonbinary" = "o", "Non Binary " = "o", "Unsure" = "o", "Non binary " = "o", "good" = "o", "Neutral" = "o", "neutral" = "o", "nonbinary" = "o", "bigender" = "o", "hen" = "o", "don't know"  = "o", "Bottom half male, nose upwards female" = "o" )) %>% 
  mutate(bbcat = ifelse(categorization == "o"|categorization == "4", 1, 0),
         fem = as.factor(fem))
tmp %>% select(categorization, condition, fem, bbcat) %>% head()
```

So how do we actually test the question "do people use the non-binary option when they have them"? We can construct several models which gets increasingly more complicated and compare how well they predict the data. So we start with a null model, m~0~, which just has an intercept, and includes neither morph level or condition. For reference, it looks like this, but feel free to let your eyes glaze over it. (but in case your curious, the model includes random effects for subjects and faces, but nothing else)

$$
\begin{aligned}
\text{beyond-binary}_{i} &\sim \mathrm{Binomial}(1,p) &  (1) \\
\text{logit}(p_i)&= \alpha_{subject[i]} +\alpha_{face[i]}\\
\alpha_{face} & \sim \mathrm{Normal(0, \sigma_{subject})}\\
\alpha_{subject} &\sim \mathrm{Normal}(0, \sigma_{subject}) \\
\sigma_{face} &\sim \mathrm{HalfCauchy}(3) \\
\sigma_{subject} &\sim \mathrm{HalfCauchy}(3) \\
\end{aligned}
$$

Then m~1~ adds fixed effect of condition and morph level. We can think about this as basically testing just the main effect of both condition (written out as $\gamma_{cid}$) and morph level ($\beta_1Morph_{i}$), but not the interaction (which we'll get to later). $cid$ is short for condition id, which means that this model calculates separate values for each condition, as opposed to a dummy variable, which would calculate the value (intercept) for one condition and then the difference of another condition. Here we just get values for each condition, and if we want to know the difference, we have to calculate it ourselves.

To add another level of complexity, we can make a third model m~2~ which includes the effects of both response option condition and morph level.

$$
\begin{aligned}
\text{beyond-binary}_{i} &\sim \mathrm{Binomial}(1,p) & (2) \\
\text{logit}(p_i)&= \gamma_{cid[i]}+ \alpha_{subject[i]} + \beta_{[1]}Morph_{i}+ \alpha_{face[i]}\\
\gamma_{cid} &\sim \mathrm{Normal}(0,3)\\
\alpha_{subject} &\sim \mathrm{Normal}(0, \sigma_{subject}) \\
\beta_{1} & \sim \mathrm{Normal}(0,3)\\
\sigma_{face} &\sim \mathrm{HalfCauchy}(3) \\
\alpha_{subject} &\sim \mathrm{HalfCauchy}(3) \\
\sigma_{\gamma_{pronoun}}&\sim \mathrm{HalfCauchy}(3) \\
\textbf{R} &\sim \mathrm{LKJcorr}(2) \\
\end{aligned}
$$

Lastly, m~3~ includes the full interaction effect. Note that all we've really done is changed $\beta$ by also adding the $cid$ subscript. What that means is that we're calculating a unique effect of morph level for each condition. Again, the model doesn't include an *explicit* interaction term (which, in standard notation, be the difference between the a reference condition and another condition), but we can calculate it later. And we will, don't worry!

$$
\begin{aligned}
\text{beyond-binary}_{i} &\sim \mathrm{Binomial}(1,p) & (3) \\
\text{logit}(p_i)&= \gamma_{cid[i]}+ \alpha_{subject[i]} + \beta_{cid[i]}Morph+ \alpha_{face[i]}\\
\gamma_{cid} &\sim \mathrm{Normal}(0,3),\: \text{for}\: cid =\text{ft, mc}\\
\alpha_{subject} &\sim \mathrm{Normal}(0, \sigma_{subject}) \\
\beta_{cid} & \sim \mathrm{Normal}(0,3),\: \text{for}\: cid =\text{ft, mc}\\
\sigma_{face} &\sim \mathrm{HalfCauchy}(3) \\
\sigma_{subject} &\sim \mathrm{HalfCauchy}(3) \\
\sigma_{\gamma_{pronoun}}&\sim \mathrm{HalfCauchy}(3) \\
\textbf{R} &\sim \mathrm{LKJcorr}(2) \\
\end{aligned}
$$

```{r, echo = FALSE, message = FALSE}
m0 <- brm(bbcat ~ 1  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
          prior = c(prior(normal(-3,3), class = "Intercept")
                    
                    #prior(normal(0,3), class ="b", coef= "conditionmc:fem"),
                    #prior(normal(0,3), class = "b", coef = "conditionft")
                    #prior(normal(0,3), class ="b", coef= "conditionft:fem")
                    ),
          data = tmp,
          iter = 6000, warmup = 2000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_binary_null.3"
          )


m1 <- brm(bbcat ~ 0 + condition + fem  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
          prior = c(prior(normal(0,3), class = "b", coef = "conditionmc"),
                    
                    #prior(normal(0,3), class ="b", coef= "conditionmc:fem"),
                    prior(normal(0,3), class = "b", coef = "conditionft")
                    #prior(normal(0,3), class ="b", coef= "conditionft:fem")
                    ),
          data = tmp,
          iter = 4000, warmup = 1000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_binary_stair_bb_fem"
          )

m2 <- brm(g_cat ~ 0 + condition:fem  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
         prior = c(prior(normal(-7,5), class = "b"),
                   prior(cauchy(0,3), class = "sd")),
          data = tmp,
          iter = 6000, warmup = 2000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_binary_stair_bb_fem_int_prior2"
          )

```

Having fit all of these models I then compare them to see which best predicts the data. We do this using a method call leave-one-out cross validation. This tells us which of our five models best predict the data on "new" or out of sample data points? If I can get the machine to work, this should show up in table 1

```{r, echo=FALSE, warning= FALSE, message = FALSE}
library(loo)
loo1 <- loo(m0)
loo2 <- loo(m1, mc_cores =4) 
loo3 <- loo(m2, mc_cores =4)

loo_table <- loo_compare(loo1, loo2, loo3)
```

```{recho=FALSE, warning= FALSE, message = FALSE}
library(knitr)
library(kableExtra)

loo_table[,1:4]

kable(
  loo_table[,1:4] %>% round(2),
  booktabs = "TRUE",
  #format = "latex",
  col.names = c("LOO difference", "St. Error diff", "LOO", "St. Error LOO"),
  #row.names = c("Free text", "Multiple categories", "Binary categories"),
  align = c("l", "c", "c", "c"),
  caption = "Relative predictive power of models describing the outcome on the categorization task"
  ) %>% 
  kable_classic(full_width = F) %>% 
  footnote(
    general_title = "Note.",
    general = "LOO diff refers to the difference in loo between the model and the most predictive model. The first row describes the most predictive model, which is why the difference is 0",
    threeparttable = TRUE,
    footnote_as_chunk = TRUE
    ) 
```

Table 1. suggests that the interaction models was the best at predicting data, suggesting that the interaction between condition and morph is an important determinant of the outcome. However, the comparatively large standard errors suggests that this effect is only inconclusively supported by the data.

#### Interaction effects

I'm going to forge ahead anyway, and do what we might think of as contrast analyses, focusing on m~3~ only. Here we have several specific questions that we want answered. First, is it really the case that people generally make more beyond-binary categorizations in the mc condition? And secondly, is this effect concentrated at 50/50 morph level. First, let's take a look at the data visualised.

```{r, echo = FALSE, message =FALSE, error=FALSE}
conditional_effects(m2)
```


```{r, echo = FALSE, message =FALSE, error=FALSE}
h0 <- hypothesis(m2,
                 "(conditionft:fem16.67 + conditionft:fem33.33 + conditionft:fem0 + conditionft:fem50 + conditionft:fem66.67 + conditionft:fem83.33)/6 =
                 (conditionmc:fem16.67 + conditionmc:fem33.33 + conditionmc:fem0 + conditionmc:fem50 + conditionmc:fem66.67 + conditionmc:fem83.33)/6 ") 

h1 <- hypothesis(m2, "conditionft:fem50=conditionmc:fem50") 
h2 <- hypothesis(m2, "(conditionft:fem0 + conditionft:fem100)/2 =(conditionmc:fem0 + conditionmc:fem100)/2") 
```

I'm going to give these hypothesis some arbitrary numbers.

First, if we just look at the number of beyond-binary categorizations. 

1. Do particpants make beyond-binary categorizations in the multiple categories condition compared to the free text condition (h0)? It seems they do (Estimate = `r h0$hypothesis[2] %>% round(2)`, CI =`r h0$hypothesis[4]%>% round(2)` - `r h0$hypothesis[5]%>% round(2)`, BF~10~= 1 `r 1/h0$hypothesis[6]%>% round(2) %>% round(2)`).

2. If we focus on the 50/50 faces, do we still see an effect? It seems like we do (Estimate = `r h1$hypothesis[2]`, CI =`r h1$hypothesis[4] %>% round(2)` - `r h1$hypothesis[5]%>% round(2)`, BF~10~= 1 `r 1/h1$hypothesis[6]%>% round(2)`). 

3. And if we focus on the faces at the end of the spectrum (i.e. 0 and 100 faces)? Here the evidence is only inconclusive (Estimate = `r h2$hypothesis[2]%>% round(2)`, CI =`r h2$hypothesis[4]%>% round(2)` - `r h2$hypothesis[5]%>% round(2)`, BF~10~= 1 `r 1/h2$hypothesis[6]%>% round(2)`). What does this mean? Let's discuss

### How does this effect the relative distribution of m/f scores

Great, let's move on to the step 2 that we talked about. So having answered the questions of "do people actually use the extra response options" with a resounding "probably, but not so much", I'm going to move on to the second question: is it the case that the answers shift the distribution of m/f scores? Spoilers, it seems maybe not. Anyway, the first thing I did was make a version of the data where all the beyond-binary responses were taken out. Sort of the opposite of what I did last. I named the relevant variable g_cat because naming things is hard. 

```{r, message=FALSE, echo=FALSE}
tmp <- d %>% 
  filter(condition == "mc"| condition == "xb"|condition == "ft" )%>% 
  mutate(categorization = recode(categorization, 
                                 "F" = "f", "kvinna" = "f", "female" = "f", "female " = "f", "Female"= "f", "Fenale"= "f", "women" = "f", "woman " = "f", "femLE" = "f", "FEmale" = "f", "Femalw" = "f", "Fwmalw" = "f", "Female " = "f", "woman" = "f", "Woman" = "f", "feMale" = "f", "fermale" = "f", "wman" = "f", "Femae" = "f",
                                 "man" = "m","Male " = "m", "make" = "m", "Male"= "m", "male" = "m", "man " = "m",  "male " = "m", "guy" = "m", "boy" = "m", "Make" = "m", "M"  = "m", "Man" = "m", "Bottom half male; above nose female., Would have to say Male" = "m", " male" = "m", "male  " = "m", "ale" = "m", "nmale" = "m", "MALE"= "m", "nale"= "m", " Male" = "m",
                                 "Nonbinary" = "o", "Non Binary " = "o", "Unsure" = "o", "Non binary " = "o", "good" = "o", "Neutral" = "o", "neutral" = "o", "nonbinary" = "o", "bigender" = "o", "hen" = "o", "don't know"  = "o", "Bottom half male, nose upwards female" = "o",
                                 "1" = "f", "2" = "m")) %>% 
  mutate(g_cat = ifelse(categorization == "f"|categorization == "m", categorization, NA),
         fem = as.factor(fem)) %>% 
  mutate(g_cat =as.numeric( g_cat == "f"))

head(tmp[,c("condition", "categorization", "g_cat")])
```

I'm tired, your tired, so I'm going to move through this a little more quickly than the previous stuff. But basically, I went through the same steps as for the previous dataset, note that now I included condition

```{r}
fit_mf_null <- brm(g_cat ~ 1  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
          prior = c(prior(normal(0,3), class = "Intercept"),
                    prior(exponential(2), class = "sd")
                    
                    #prior(normal(0,3), class ="b", coef= "conditionmc:fem"),
                    #prior(normal(0,3), class = "b", coef = "conditionft")
                    #prior(normal(0,3), class ="b", coef= "conditionft:fem")
                    ),
          data = tmp,
          iter = 6000, warmup = 2000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_mf_null"
          )


fit_mf_cond <- brm(g_cat ~ 1 + condition  + (1 |id) + (1|face), family = bernoulli(link = 'logit'), 
          prior = c(prior(normal(0,3), class = "b"),
                    prior(cauchy(0,1), class = "sd")),
          data = tmp,
          iter = 6000, warmup = 1000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_mf_cond"
          )

fit_mf_mainfx <- brm(g_cat ~ 1 + condition + fem  + (1 |id) + (1|face), family = bernoulli(link = 'logit'), 
          prior = c(prior(normal(0,3), class = "b"),
                    prior(cauchy(0,3), class = "sd")),
          data = tmp,
          iter = 4000, warmup = 1000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_mf_mainfx"
          )

fit_mf_int <- brm(g_cat ~ 0 + condition:fem  + (1 |id) + (1|face), family = bernoulli(link = 'logit'), 
         prior = c(prior(normal(0,3), class = "b"),
                   prior(cauchy(0,3), class = "sd")),
          data = tmp,
          iter = 6000, warmup = 2000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_mf_int"
          )

fit_mf_morph <- brm(g_cat ~ 1 + fem  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
         prior = c(prior(normal(0,3), class = "b"),
                   prior(normal(0,3), class = "Intercept"),
                   prior(cauchy(0,3), class = "sd")),
          data = tmp,
          iter = 6000, warmup = 2000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_g_fem"
          )


```

We're going to do the same thing again, we're going to start buy looking at whether the interaction model does better than the simple effects model, and then we're going to look at the actual effects within the model. Buuut. It doesn't seem lik it does much here.

```{r,}
loo_b1 <- loo(fit_mf_int, mc_cores = 4)
loo_b2 <- loo(fit_mf_null, mc_cores = 4)
loo_b3 <- loo(fit_mf_cond, mc_cores = 4)
loo_b4 <- loo(fit_mf_mrph, mc_cores = 4)
loo_b5 <- loo(fit_mf_mainfx, mc_cores = 4)


loo_compare(loo_b1, loo_b2, loo_b3, loo_b3.5, loo_b4)
```

Okay, I took a peek at the data and it really doesn't like like anything to me. By which i mean that it doesn't look like we can say with certainty that any gender category is being shaved of at all. So it doesn't seem like any of these models add much that the the null doesn't
