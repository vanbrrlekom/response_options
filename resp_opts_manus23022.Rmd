---
title             : "The effect of response options on gender categorization ( provisional title)"
shorttitle        : "Rsponse options and gender categorization"

author: 
  - name          : "Elli van Berlekom"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Albanovägen 12"
    email         : "elli.vanberlekom@psychology.su.se"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Coauthors"
    affiliation   : "1,2"
    role:
      - "A lot of things"
      - "Author order TBD"

affiliation:
  - id            : "1"
    institution   : "Stockholm University"
  - id            : "2"
    institution   : "Lund University"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Data & scripts are available at osf link. The authors declare no conflict of interest.

abstract: |
  Gender categorizations studies almost exclusively measure outcomes in terms of woman and men only. However, this does not capture the diversity of gender and may produce inaccurate results. Consequently, this study investigated the effect of response options on binary gender categorization (N = 120). The findings suggest that participants are more likely to categorize gender beyond the binary when provided with explicit response options, such as "non-binary" and "I don't know". The results are consistent with previous research on the importance of including flexible response options to better express oneself. However, it was also found that increased freedom did not necessarily increase gender diversity in participants' categorizations. The study highlights the importance of careful consideration of response options in gender categorization research. Future research may benefit from utilizing a variety of response options, including open text-boxes, forced choice-alternatives, and dimensional scales.

  
keywords          : "Gender diversity, Gender categorization, Transgender, Measurement"
wordcount         : "3557"

bibliography      : "r-references.bib"

floatsintext      : no
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

header-includes:
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"
output            : papaja::apa6_pdf
always_allow_html: true
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")

library(brms)
library(tidyverse)
source("src/functions.r")
d  <- read_and_clean("data/cat2stduy1_data.csv") %>% 
  mutate(fem = 50-masc) 
getwd()

```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Precision is key when measuring constructs in psychological research. One domain where precision is lacking is in the use of binary  response options for gender (i.e. "woman" and "man"), which fails to capture the complex and fluid nature of gender/sex [@hyde_future_2018; @lindqvist_what_2020]. While researchers have begun offering participants more flexible options to define their gender [@lindqvist_what_2020; @saperstein_categorical_2021], such as additional categories or free text responses, studies still predominantly use binary gender options when categorizing participants. Thus, understanding of how participants' response measures affect gender categorization remains limited Therefore, the current study aims to test how non-binary alternatives affect gender categorization.

@bem_measurement_nodate was a pioneer in challenging the dichotomous and binary conceptualizations of gender. Bem  developed the first scale (the Bem Sex Role inventory, BSRI, 1974) to measure femininity and masculinity as separate traits, finding that many individuals exhibit a mixture of feminine and masculine traits. The BSRI has been used more often as self-reports of gender as a trait. More recently, several guides have been developed that recommend the use of open free text options or multiple options (e.g., woman, man, other, nonbinary). when asking participants to indicate their gender (see for example Lindqvist et al., 2020). When participants have given such options, they use them to indicate a gender identity other than women or men. Although this practice is still far from the norm, it is becoming increasingly commonplace for researchers to measure gender of participants in this open-ended way [@carleton_assessing_2022; @cronin_younger_2022; @dagostino_organizational_2022; @gottgens_impact_2022]

In contrast, the literature on gender categorization of others still almost exclusively treats gender as a binary category. Gender categorization is a cognitive process that occurs when individuals perceive others [@kang_multiple_2015]. Researchers in this field have explored the speed and automaticity of gender perception in faces, as well as which facial features are associated with specific gender categories, such as women and men [e.g. @mogilski_relative_2018] . Generally, the findings indicate that gender is rapidly and automatically categorized [@habibi_spontaneous_2012; @jung_automaticity_2019], with facial features such as skin smoothness, jawline, and hair length used to determine gender identity. Lastly, studies in this field have indicated that people perceive faces categorically [@campanella_categorical_2001]. In other words, that faces, even when manipulated to vary on a continuum from feminine to masculine were still perceived as belonging to the categories women or man. However, these studies typically do reflect the diverse nature of gender or consider alternative response options (ref).

Instead, gender categorization is most often measured through a forced-choice task in which participants are forced to indicate either "female" or "male" when presented with a face [see for example @campanella_categorical_2001; @cloutier_perceptual_2005; @webster_adaptation_2004; @zhao_own-_2008]. A slightly different approach asks participants to rate faces on a gender gradation as a quality, often using "feminine" and "masculine" as endpoints on a single gradation [@dascenzo_imagining_2014]. Only in some rare cases are masculinity and femininity measued using different gradations for femininity and masculinity [e.g. @wittlin_about_2018]. Despite some variations therefore, the literature overwhelmingly presents gender as a binary in studies of gender categorization of others.

The use of binary gender measures presents significant problems in accurately reflecting the diversity of gender. Such measures reinforce the notion of gender as a binary concept, thereby invalidating non-binary genders. This not only misrepresents the reality of gender but also perpetuates discriminatory attitudes towards non-binary individuals. Consequently, it is imperative to test alternative measures that acknowledge and respect the diversity of gender identities.

## Overview of the present research

Across two experiments, we investigate how alternative response options impact gender categorization. The purpose of both experiment was to answer the following three research questions 
*Research question 1*: Do participants categorize gender beyond the binary when response options allow them to do so?
*Research question 2*: Two what extent do beyond-binary responses affect the distribution of woman/man responses?
*Research question 3*: Can response options which do not present the categories of woman and man as oppositional reduce categorical perception. 

Research questions 1 and 2 were investigated in study 1 and research question 3 was investigated in study 2. 

# Study 1

Study 1 tested whether response options that allow categorization beyond the binary influences people to categorize gender beyond the binary. Accordingly, Experiment 1 manipulated response options.The specific alternatives were based on common practices for self-identification of gender. To avoid suggesting that gender only consists of women and men, these studies recommend including a third option. Moreover, given that gender may not always be evident from a person’s countenance, an “I don’t know” option should also be incorporated into the task. This suggests that a forced choice task is best supplemented with both an “other” option and an “I don’t know” option. Another common practice is to give people an open text box in which they may type in whatever they like. This method has been recommended by different researchers [@lindqvist_what_2020; @saperstein_categorical_2021]. This method has the benefit of affording participants freedom to answer as they choose.Therefore the study included three conditions (Binary categories, Multiple  categories and Free text options). 

# Method

## Participants

```{r}
subs <- d %>%
  filter(condition == "ft" | condition == "xb" | condition == "mc") %>% 
  mutate(age = as.numeric(Age.1),
         gender = substr(Gender.1, 1,1) ) %>% 
  count(id, age, gender)

```


Swedish participants (*N* = `r length(subs$id)`)  were recruited through advertising online and on the university campus (*M*~age~= `r mean(subs$age, na.rm = TRUE)`,  *SD*~age~ = `r sd(subs$age, na.rm = TRUE)`, Range = `r min(subs$age, na.rm = TRUE)` - `r max(subs$age, na.rm = TRUE)`). Self-identified gender was measured using an open-ended text box; participants were 35 women, 32 men and 1 who did not indicate gender). All participants were informed that participation was voluntary, that they could withdraw from the study and that results do no include any identifying features. In accordance with ethical guidelines, all participants provided written informed consent.



## Stimuli 

Faces used for the gender categorization task were produced using the London Face Database (deBruine) and the  Chicago Face Database (ref) morphed with on Webmorph (ref). For Black, Asian and White faces, the six most feminine faces of women and the six most masculine faces of men were selected, using the codebook provided by the researchers. The faces were matched, so that the most feminine faces in the database were morphed with the most masculine faces. The morphs were made in 7 steps, from completely feminine to completely masculine. Because there were 18 pairs morphed in 7 steps, the total number of faces was 126. Each pair of faces were morphed in 7 steps, from completely feminine to completely masculine resulting in a  total number of 126 faces (see Figure\ \@ref(fig:insert), which should include percentage femininity of the face ) 

## Measures

The primary outcome was responses to the categorization task. For analysis purposes, these were aggregated in the following ways:

*Beyond-binary categorizations* represented the categories where participants did not categorise the face as woman or man. This was a dichotomous variable that was calculated from the categorization data by combining the responses of “I don’t know” and “non-binary”. These beyond-binary responses were coded as 1 and binary responses as 0.

*Binary categorization* represented only the responses that were either woman (coded as 1) or man (coded as 0). All other responses were removed from this dataset.

## Procedure

Participants completed the experiment on a computer in a quiet room. Each trial consisted of a face accompanied by the question "How would you gender categorize this person?". Each person completed a total of 126 trials (i.e. they categorized every face in the stimuli set). Participants were randomly allocated into one of the three response options conditions: binary categories, multiple categories, and free text. In the binary categories condition, the only option to respond was “woman” and “man”. he multiple categories conditionincluded the options “other” and “I don’t know” as well as woman or man. the free text condition consisted of an open text box.


## Data analysis

We used `r cite_r("r-references.bib")`. Descriptive statistics were used to summarize the data, and Bayesian mixed-effects models were used to test the research questions.  For all models, we included varying intercepts for both participants and trials. To answer each research question, we used a two-step approach which began with a model comparison approach followed by Bayes factor tests of specific contrasts. In all cases, the models included varying intercepts for both participants trials.  

### Research question 1: The use of categories beyond the binary

In research question one, we investigated whether participants categorized faces beyond the binary when given the chance. This could manifest as either a main effect of condition or an interaction between condition and morph level if categorizations beyond the binary were limited to only the most androgynous faces. For this analyses, the Binary categories condition was excluded, as that condition precluded the possibility of categorizing beyond the binary. The specific questions then, were "do people categorize faces beyond the binary?", "does this effect depend on condition?" and "are ambiguous faces more likely to be categorized beyond the binary?". These questions correspond to main effects of response option condition, facial morph level and an interaction between the two. 

Accordingly, the models were fit to the outcome *Beyond-binary categorizations* and comprised a Null model with no additional predictors, a Main effects model and an Interaction model  For full model specification (including priors) and model diagnostics, see the supplementary material. These model were then compared in terms of predictive power on out-of-sample data points, estimated using Leave-one-out cross validation (LOO-CV). This represents an indirect test of the research questions, and can be viewed as an imperfect analogy to checking whether there is a "significant" overall interaction in a classical F-test.

As a more direct test, we also calculated the Bayes Factor for the specific contrasts suggested by these questions. In other words, we compared the overall probability of making categorizations beyond the binary in the Free text condition and the Multiple categories conditions. Additionally,
we compared the prevalence of categorization beyond the binary specifically of the most androgynous faces. The Bayes factors were compared the null hypothesis that the contrast was equal to 0 and calculated using the Savage-Dickey Density Ratio. 

### Research question 2: The distribution of binary responses 

In research question two, we investigated whether the distribution of *binary responses* was different depending on response option condition. This could manifest as a main effect of condition if there was an overall skew in the results or as an interaction between condition and morph level, in case that the skew was isolated to just one level of morph (for example at the middle).

Similar to RQ1, this data was tested using with Bayesian mixed models fitted to the data. This included an initial model comparison approach, with Null model, and Main Effects model and an Interaction Model. If the model comparison did not preclude the Interaction model, we tested the contrast of the overall distribution as well as isolated to whichever morph level, a visual inspection of the data suggested was the most strongly skewed.

# Results

The raw distribution of gender categorizations made by participants is presented in Figure\ \@ref(fig:descriptives).

*to do: fix the bug that is producing those ugly red lines at the bottom of this figure*

```{r descriptives, fig.cap= "Gender Categorizations by Participants"}

d %>%
  mutate(fem = fem + 50) %>% 
  filter(condition == "ft" | condition == "xb" | condition == "mc") %>% 
  group_by(fem, race, condition) %>% 
  mutate(categorization = recode(categorization,  #yes, this is pretty horrendous code, I haven't had a chance to sit down and clean it up yet.
                                 "1" = "Woman", "Woman" = "Woman", "kvinna" = "Woman", "female" = "Woman", "female " = "Woman", "Female"= "Woman", "Fenale"= "Woman", "women" = "Woman", "woman " = "Woman", "femLE" = "Woman", "FEmale" = "Woman", "Femalw" = "Woman", "Fwmalw" = "Woman", "Female " = "Woman", "woman" = "Woman", "Woman" = "Woman", "feMale" = "Woman", "fermale" = "Woman", "wman" = "Woman", "Femae" = "Woman",
                                 "2" = "Man", "man" = "Man","Male " = "Man", "make" = "Man", "Male"= "Man", "male" = "Man", "man " = "Man",  "male " = "Man", "guy" = "Man", "boy" = "Man", "Make" = "Man", "Man"  = "Man", "Man" = "Man", "Bottom half male; above nose female., Would have to say Male" = "Man", " male" = "Man", "male  " = "Man", "ale" = "Man", "nmale" = "Man", "MALE"= "Man", "nale"= "Man", " Male" = "Man",
                                 "3" = "Other", "Nonbinary" = "Other", "Non Binary " = "Other", "Unsure" = "Other", "Non binary " = "Other", "good" = "Other", "Neutral" = "Other", "neutral" = "Other", "nonbinary" = "Other", "bigender" = "Other", "hen" = "Other", "don't know"  = "Other", "Bottom half male, nose upwards female" = "Other",
                                 "4" = "Don't know", ),
         condition = recode(condition, "ft" = "Free text", "xb" = "Binary Categories", "mc" = "Multiple Categories"))%>% 
  count(categorization) %>% 
  ggplot(aes(x=fem, y=n, fill=factor(categorization, levels = c("Man", "Other", "Don't know", "Woman")))) +
  geom_bar(stat="identity", position = "fill") + 
  ggtitle("Gender Categorizations by Participants")+ 
  facet_wrap(~condition) + 
  scale_x_continuous(breaks =c(0, 17, 33, 50, 66, 83, 100)) +
  ylab("Proportion of responses" ) +
  xlab("Proportion female face in the morph") +
  #scale_fill_discrete(name = "Response") +
  #scale_fill_viridis_d(name = "Response")
  scale_fill_grey( name = "Response")+
  theme_apa()
 
```


To investigate whether response options affected gender categorization  we fit a Null Model, a Main Effects Model and an interaction model to the data. As stated, for these analyses, the Binary Categories condition was excluded, as participant did not have the option to categorize beyond the binary. The results of model comparison are presented in Table\ \@ref(tab:loo). 
Table\ \@ref(tab:loo) suggests that the Interaction model is the most predictive. However the absolute difference between the Interaction model and the Main effects model is small and importantly, the difference is small in relation to the standard error of the difference. This suggests that the data is inconclusive about which model is most suitable, although both are superior to the Null model. As model comparison did not conclusively preclude the Interaction model, we continued by testing specific, relevant contrasts using the Interaction model (see the Supplementary material for specific contrast weights).
 

```{r, modelfit}
Null <- brm(bbcat ~ 1  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
          prior = c(prior(normal(-3,3), class = "Intercept"),
                    
                    #prior(normal(0,3), class ="b", coef= "conditionmc:fem"),
                    #prior(normal(0,3), class = "b", coef = "conditionft")
                    #prior(normal(0,3), class ="b", coef= "conditionft:fem")
                    prior(cauchy(0,3), class = "sd")
                    ),
          data = tmp,
          iter = 6000, warmup = 2000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/binary_null"
          )


Main_Effect <- brm(bbcat ~ 0 + condition + fem  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
          prior = c(prior(normal(0,3), class = "b", coef = "conditionmc"),
                    
                    #prior(normal(0,3), class ="b", coef= "conditionmc:fem"),
                    prior(normal(0,3), class = "b", coef = "conditionft"),
                    #prior(normal(0,3), class ="b", coef= "conditionft:fem")
                    prior(cauchy(0,3), class = "sd")
                    ),
          data = tmp,
          iter = 4000, warmup = 1000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/binary_mfx"
          )

Interaction <- brm(bbcat ~ 0 + condition:fem  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
         prior = c(prior(normal(-7,5), class = "b"),
                   prior(cauchy(0,3), class = "sd")),
          data = tmp,
          iter = 6000, warmup = 2000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/binary_int"
          )

```

```{r, loo}
library(loo)
#loo1 <- loo(null) %>% saveRDS("loo1.rds")
#loo2 <- loo(main_effect, mc_cores =4) %>% saveRDS("loo2.rds")
#loo3 <- loo(interaction, mc_cores =4) %>% saveRDS("loo3.rds")

loo1 <- readRDS("models/loo1.rds")
loo2 <- readRDS("models/loo2.rds")
loo3 <- readRDS("models/Loo3.rds")

loo_table <- loo_compare(loo1, loo2, loo3)  
row.names(loo_table) = c("Interaction", "Main Effect", "Null")

library(knitr)
library(kableExtra)

kable(
  loo_table[,1:4] %>% round(2),
  booktabs = "TRUE",
  format = "latex",
  col.names = c("LOO diff", "St. Error diff", "LOO", "St. Error LOO"),
  align = c("c", "c", "c", "c"),
  caption = "Relative predictive power of models describing the outcome on the categorization task"
  ) %>% 
  kable_classic(full_width = F) %>% 
  footnote(
    general_title = "Note.",
    general = "LOO diff refers to the difference in loo between the model and the most predictive model. The first row describes the most predictive model, which is why the difference is 0",
    threeparttable = TRUE,
    footnote_as_chunk = TRUE
    ) 
```

```{r exp-one-inf, fig.cap= "Proportion of beyond-binary responses in the Multiple Categories and Free Text conditions"}
# Use brms 
c_eff <- conditional_effects(Interaction) 

df <- as.data.frame(c_eff[["condition:fem"]])

ggplot(df,aes(x = fem, y=estimate__, group=condition))+
  geom_point(aes(color=condition), position = position_dodge(0.4))+
  geom_errorbar(aes(ymin=lower__, ymax=upper__, color = condition), position = position_dodge(0.4), width = 0.3) + 
  scale_color_grey(name = "Condition",
                   labels = c("Free text", "Multiple Conditions"))+
  ylab("Proportion of responses" ) +
  xlab("Proportion female face in the morph")+
  theme_apa()
  
```

```{r, calculating for tests, message =FALSE, error=FALSE}
h0 <- hypothesis(Interaction,
                 "(conditionft:fem16.67 + conditionft:fem33.33 + conditionft:fem0 + conditionft:fem50 + conditionft:fem66.67 + conditionft:fem83.33 +  conditionft:fem100)/7 =
                 (conditionmc:fem16.67 + conditionmc:fem33.33 + conditionmc:fem0 + conditionmc:fem50 + conditionmc:fem66.67 + conditionmc:fem83.33 + conditionmc:fem100)/7 ") 

h1 <- hypothesis(Interaction, "conditionft:fem50=conditionmc:fem50") 
h2 <- hypothesis(Interaction, "(conditionft:fem0*(-5) +conditionft:fem16.67 *0 + conditionft:fem33.33 *3  + conditionft:fem50 * 4 + conditionft:fem66.67 *3 + conditionft:fem83.33 *0+  conditionft:fem100 * (-5))/84 =
                 (conditionmc:fem16.67*0 + conditionmc:fem33.33 *3 + conditionmc:fem0*(-5) + conditionmc:fem50*4 + conditionmc:fem66.67 *3 + conditionmc:fem83.33*0 + conditionmc:fem100*(-5))/84 ")




```

Model parameters are visualized in Figure\ \@ref(fig:exp-one-inf). First, whether participants overall made more beyond-binary categorizations in the multiple categories condition than in the free text condition. Although the incidence was rare in both conditions () The evidence suggests fairly convincingly that this is the case (OR = `r exp(h0$hypothesis$Estimate %>% round(2))`, CI =[`r exp(h0$hypothesis$CI.Lower%>% round(2))`, `r exp(h0$hypothesis$CI.Upper %>% round(2))`], BF~10~= `r round(1/h0$hypothesis$Evid.Ratio , 2) `). Additionally, based on visual inspection of Figure\ \@ref(fig:exp-one-inf), which suggested that the difference between the condition was concentrated at morphs containing equal levels of femininity and masculinity (i.e. morph level 50) we explored whether the evidence supported this difference at morph level 50. The evidence was in favor of this difference (OR = `r exp(h1$hypothesis$Estimate %>% round(2))`, CI =[`r exp(h1$hypothesis$CI.Lower %>% round(2))`, `r exp(h1$hypothesis$CI.Upper %>% round(2))`], BF~10~=  `r round(1/h1$hypothesis$Evid.Ratio,2)`). Lastly, we tested the difference using quadratic weights, though here the difference was inconclusive  (OR = `r exp(h2$hypothesis$Estimate %>% round(2))`, CI = [`r exp(h2$hypothesis$CI.Lower %>% round(2))`,`r exp(h2$hypothesis$CI.Upper %>% round(2))`], BF~10~=  `r round(1/h2$hypothesis$Evid.Ratio,2 )`). 

Overall the evidence suggests at least somewhat strongly that when participants have the option of using beyond-binary response options, they use them. 

Subsequently, we tested whether the inclusion of non-binary response options skewed the distribution of categorization of faces as women and men. For this analysis, therefore, we tested the outcome variable *binary categorization*, excluding "other" "I don't know" and any other respones in the free text.

```{r, rq2-modelling, message=FALSE}
#Futher cleaning up the data
tmp <- d %>% 
  filter(condition == "mc"| condition == "xb"|condition == "ft" )%>% 
  mutate(categorization = recode(categorization, 
                                 "F" = "f", "kvinna" = "f", "female" = "f", "female " = "f", "Female"= "f", "Fenale"= "f", "women" = "f", "woman " = "f", "femLE" = "f", "FEmale" = "f", "Femalw" = "f", "Fwmalw" = "f", "Female " = "f", "woman" = "f", "Woman" = "f", "feMale" = "f", "fermale" = "f", "wman" = "f", "Femae" = "f",
                                 "man" = "m","Male " = "m", "make" = "m", "Male"= "m", "male" = "m", "man " = "m",  "male " = "m", "guy" = "m", "boy" = "m", "Make" = "m", "M"  = "m", "Man" = "m", "Bottom half male; above nose female., Would have to say Male" = "m", " male" = "m", "male  " = "m", "ale" = "m", "nmale" = "m", "MALE"= "m", "nale"= "m", " Male" = "m",
                                 "Nonbinary" = "o", "Non Binary " = "o", "Unsure" = "o", "Non binary " = "o", "good" = "o", "Neutral" = "o", "neutral" = "o", "nonbinary" = "o", "bigender" = "o", "hen" = "o", "don't know"  = "o", "Bottom half male, nose upwards female" = "o",
                                 "1" = "f", "2" = "m")) %>% 
  mutate(f_cat = ifelse(categorization == "f"|categorization == "m", categorization, NA),
         fem = as.factor(fem)) %>% 
  mutate(f_cat =as.numeric( f_cat == "f"))

#Modelling
Null <- brm(f_cat ~ 1  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
          prior = c(prior(normal(0,3), class = "Intercept"),
                    prior(exponential(2), class = "sd")
                    
                    #prior(normal(0,3), class ="b", coef= "conditionmc:fem"),
                    #prior(normal(0,3), class = "b", coef = "conditionft")
                    #prior(normal(0,3), class ="b", coef= "conditionft:fem")
                    ),
          data = tmp,
          iter = 6000, warmup = 2000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_mf_null"
          )



main_effects <- brm(f_cat ~ 0 + condition + fem  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
          prior = c(prior(normal(0,3), class = "b"),
                    prior(cauchy(0,3), class = "sd")),
          data = tmp,
          iter = 4000, warmup = 1000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/mainfx.2"
          )

interaction <- brm(f_cat ~ 0 + condition:fem  + (1 |id) + (1|face:fem), family = bernoulli(link = 'logit'), 
         prior = c(prior(normal(0,3), class = "b"),
                   prior(cauchy(0,3), class = "sd")),
          data = tmp,
          iter = 6000, warmup = 2000,
          chains = 4,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_mf_int.2"
          )

```

```{r, rq2-table, fig.cap= "placeholder table caption. REPLACE"}
#loo_b1 <- loo(Null, mc_cores = 4) %>% saveRDS("loo_b2.rds")
#loo_b3 <- loo(condition_only, mc_cores = 4) %>% saveRDS("loo_b3.rds")
#loo_b4 <- loo(main_effects, mc_cores = 4) %>% saveRDS("loo_b4.rds")
#loo_b5 <- loo(interaction, mc_cores = 4) %>% saveRDS("loo_b5.rds")

loo_b1 <- readRDS("models/loo_b1.rds")
loo_b2 <- readRDS("models/loo_b2.rds")
loo_b3 <- readRDS("models/loo_b3.rds")
loo_b4 <- readRDS("models/loo_b4.rds")
loo_b5 <- readRDS("models/loo_b5.rds")




loo_table2 <- loo_compare(loo_b1, loo_b2, loo_b3, loo_b5, loo_b4)


library(knitr)
library(kableExtra)
kable(
  loo_table2[,1:4] %>% round(2),
  booktabs = "TRUE",
  #format = "latex",
  col.names = c("LOO difference", "St. Error diff", "LOO", "St. Error LOO"),
  #row.names = c("Free text", "Multiple categories", "Binary categories"),
  align = c("l", "c", "c", "c"),
  caption = "Relative predictive power of models describing the outcome on the categorization task"
  ) %>% 
  kable_classic(full_width = F) %>% 
  footnote(
    general_title = "Note.",
    general = "LOO diff refers to the difference in loo between the model and the most predictive model. The first row describes the most predictive model, which is why the difference is 0",
    threeparttable = TRUE,
    footnote_as_chunk = TRUE
  )

```

```{r hypos 2}
h0 <- hypothesis(interaction, "conditionft:fem50 = conditionmc:fem50")
h1 <- hypothesis(interaction, "conditionxb:fem50 = conditionmc:fem50")




```



To test this research question, we first carried out model comparison. The results of this are presented in Table\ \@ref(tab:rq2-table). Again, we compared a Null model, a Main Effects model and an Interaction model. Although the Interaction model was the worst in terms of LOO-CV, the standard errors were quite large relative to the difference, again suggesting that the model comparison was inconclusive and the existance of an interaction could not be excluded. For completeness we therefore carried out the contrast analyses using the Interaction model. 

Based on the pattern in Figure\ \@ref(fig:descriptives), which seems to show that in the Multiple Categories condition, participants made fewer Man responses compared to the other condition, we compared the distribution of woman/man responses at morph level 50. The evidence were slightly in favor of there being no difference between the multiple categories and the free text conditions (OR = `r exp(h0$hypothesis$Estimate %>% round(2))`, CI =[`r exp(h0$hypothesis$CI.Lower%>% round(2))`, `r exp(h0$hypothesis$CI.Upper %>% round(2))`], BF~01~= `r round(h0$hypothesis$Evid.Ratio , 2) `) and moderately in favor of no difference between multiple categories and binary categories conditions (OR = `r exp(h1$hypothesis$Estimate %>% round(2))`, CI =[`r exp(h1$hypothesis$CI.Lower%>% round(2))`, `r exp(h1$hypothesis$CI.Upper %>% round(2))`], BF~01~= `r round(h1$hypothesis$Evid.Ratio , 2) `). In other words, the evidence suggests that when participants categories faces beyond the binary, this does not skew the categorizations of women and men in any direction.

# Discussion

Eexperiment 1 indicated that participants categorize beyond the binary when response options include more options than women and men only.  However, the free text option did not differ from the binary option. Thus, the written out choices seem to act as reminders to participants.  Furthermore,  categorization beyond the binary affected former man and women responses to similar degrees, meaning that the ratio of women and men were still about 50/50. This did not systematically affect their overall pattern of responses in terms of woman and man categorizations.


# Study 2

Study  2 tested whether continuous scales that measure woman and men separately reduce categorical perception. To that end, we once again borrowed from the literature on self-categorization, this time using Bem’s (1978) method of measuring gender on two separate scales. 
If categorical perception occurs, ratings of woman and man should be skewed near the 50/50 morph level. In other words, a face with 33.33% woman would be rated as less woman than that. Therefore, we examined the differences between the two conditions at morph levels 33.33% and 66.67% morph.


# Method

### Participants

Participants (*N* = 49)  were recruited through advertising online and on the university campus (*M*~age~= 36.67,  *SD*~age~ = 12.54).Self-identified gender was measured using an open-ended text box; 25 women and 24 men participated. Participants were monetarily compensated for their time. All participants were informed that participation was voluntary and gave written consent to participate in the study in accordance with ethical recommendations. The participants were randomly allocated to conditions. 

## Stimuli & Procedure

The stimuli and procedure for study 2 were identical to experiment 1 but included only two conditions. Study 2 differed only the response options conditions, such that esponse option conditions consisted of a single dimension, which ranged from “woman” to “man” and “multiple dimension” which ranged from “not woman” to “woman” and “not man” to “man”. For the multiple dimensions condition, participants rated the same faces according to both scales, but on separate trials. Although Bem (1978) used scales of femininity and masculinity, the anchors were chosen because gender categorization was the focus of the present study.

## Data analysis

Research question 3 asked whether whether participants would view faces less categorically in when they categorized them using multiple dimensions (i.e. Not woman - Woman/Not man -Man) than when they used the single dimension (Woman-Man). We expected to see a difference between the two conditions at morph levels 33.33 and 66.67 as these were the closes to the midwaty point. In other words, if categorical perception is reduced, we would expect to see an interaction between  condition and morph level, but not a main effect. To test this, we fit a single Bayesian mixed-effects model which calculated unique fixed intercepts at each intersection of morph level and condition as well as varying intercepts for participants and faces (See supplemental material for full model specification). Using Savage-Dickey density ratios, we calculated the Bayes Factors for the contrasts between single dimension condition and multiple dimension at morph level 33.37 and 66.66 only. 

# Results

The mean ratings in both conditions are presented in Figure\ \@ref(fig:descriptives-two). 

```{r descriptives-two, fig.cap= "Mean ratings of faces in Single dimension and multiple dimensions"}
d %>% 
  filter(condition == "md" | condition == "sd") %>% 
  mutate(categorization = as.numeric(categorization) %>% ifelse(scale == "m" | condition == "sd", 100-., .),
         scale = recode(scale, "1" = "Woman - Man", "f" = "Not Woman - Woman", "m" = "Not Man - Man"),
         condition = recode(condition, "sd" = "Single Dimension", "md" = "Multiple Dimensions")) %>% 
  group_by(fem, scale, condition) %>% 
  summarise(mean_rating = mean(categorization)) %>% 
  ggplot(aes(x=fem, y=mean_rating, group = scale)) +
  geom_line(aes(color = scale))+
  geom_point(aes(color = scale))+
  theme_minimal()+
  facet_wrap(~condition) +
  scale_x_continuous(breaks =c(0, 17, 33, 50, 67, 83, 100)) +
  ylab("Mean \"woman\" rating" ) +
  xlab("Proportion female face in the morph") +
  scale_color_grey(name = "Condition")+
  theme_apa() 
  
```

```{r, fig.cap= "Mean rating of woman across the md and sd condition"}
#Wrangle data
tmp <- d %>% 
  filter(condition == "sd" | condition == "md") %>% 
  mutate(f_rating = as.numeric(categorization) %>%  ifelse(scale == "f", ., 100- .),
         scale_new = ifelse(scale == "f" | scale =="m", scale, "sd"),
         fem = as.factor(fem)) 


fit_dimensional <- 
  brm(f_rating ~ 0 + fem:condition + (1 + masc|id) + (1|face), family = gaussian(link = 'identity'), 
          prior = c(prior(normal(50,50), class = "b"),
                    #prior(normal(50,50), class = "Intercept"),
                    prior(exponential(1), class = "sd"),
                    prior(lkj(1), class = "cor"),
                    prior(exponential(1), class = sigma)),
          data = tmp,
          iter = 4000, warmup = 1000,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_dimensional_stair_factor.3")

fit_dimensional_2 <- 
   brm(f_rating ~ 1 + condition*fem + (1 + masc|id) + (1|face), family = gaussian(link = 'identity'), 
          prior = c(prior(normal(0,50), class = "b"),
                    prior(normal(50,50), class = "Intercept"),
                    prior(exponential(1), class = "sd"),
                    prior(lkj(1), class = "cor"),
                    prior(exponential(1), class = sigma)),
          data = tmp,
          iter = 4000, warmup = 1000,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_dimensional_stair_factor4")


fit_dimensional_null <- 
  brm(f_rating ~ 1+ (1 + masc|id) + (1|face), family = gaussian(link = 'identity'), 
          prior = c(#prior(normal(50,50), class = "b"),
                    #prior(normal(50,50), class = "Intercept"),
                    prior(exponential(1), class = "sd"),
                    prior(lkj(1), class = "cor"),
                    prior(exponential(1), class = sigma)),
          data = tmp,
          iter = 4000, warmup = 1000,
          cores = 4,
          sample_prior = TRUE,
          file = "models/fit_dimensional_null1")

# carrying out the hypothesis test
h_dim_33 <- hypothesis(fit_dimensional, "fem33.33:conditionmd= fem33.33:conditionsd" )
h_dim_67 <- hypothesis(fit_dimensional, "fem66.67:conditionmd= fem66.67:conditionsd" )
```

```{r exp-two-inf, fig.cap= "Mean gender ratings in Single Dimension and Multiple Dimensions conditions"}
# Use brms 
c_eff <- conditional_effects(fit_dimensional) 

df <- as.data.frame(c_eff[["fem:condition"]])

ggplot(df,aes(x = fem, y=estimate__, group=condition))+
  geom_line(aes(color=condition), position = position_dodge(0.4)) +
  geom_point(aes(color=condition), position = position_dodge(0.4))+
  geom_errorbar(aes(ymin=lower__, ymax=upper__, color = condition), position = position_dodge(0.4), width = 0.3) + 
  scale_color_grey(name = "Condition",
                   labels = c("Single Dimension", "Multiple Dimensions"))+
  ylab("Mean rating" ) +
  xlab("Proportion female face in the morph")+
  theme_apa()
  
```

We compared the mean rating at 33.33 morph and at 66.67 morph for both conditions. At 33.33 the evidence strongly suggested that the two conditions were the same
(Estimate = `r h_dim_33 $hypothesis$Estimate %>% round(2)`, CI =[`r h_dim_33 $hypothesis$CI.Lower%>% round(2)`, `r h_dim_33 $hypothesis$CI.Upper %>% round(2)`], BF~01~= `r round(h_dim_33 $hypothesis$Evid.Ratio , 2) `). This was also the case at 66.67 
(Estimate = `r h_dim_67$hypothesis$Estimate %>% round(2)`, CI =[`r h_dim_67$hypothesis$CI.Lower%>% round(2)`, `r h_dim_67$hypothesis$CI.Upper %>% round(2)`], BF~01~= `r round(h_dim_67$hypothesis$Evid.Ratio , 2) `). Overall, both conditions showed fairly strong tendencies toward categorical perception and they did not differ in this regard. 

# Discussion

Experiment 2 showed that r response scales which did not present women and men as opposing categories did not changed participants categorical perception. Indeed a highly binary view of gender was present and participants treated womanhood and manhood as opposites although the scale would allow them to be more flexible. 

# General discussion

In two experiments, we tested how response options in gender categorization of others influence binary gender categorization.  Specifically, the results provided strong evidence that participants only use beyond-binary options to categorize faces when such options are provided explicitly. Free text answers or continuous scales did not affect participants binary gender categorization. 

These findings are somewhat consistent with previous research, such as the work of Saperstein and Westbrook (2018) and Lindqvist (2019), which has shown that including flexible response options allow participants to better express themselves. Unlike the literature on self-categorization, increased freedom did not increase the gender diversity of participants' categorizations, rather explicit reminders seem to have the largest effect. When participants categorized women or men on continuous scales, the results differ from Bem’s (1978) who found that participants categorize their own femininity and masculinity independently of each other. Rather, when categorizing others, the participants in the present study seemed to treat women and men as opposites, even when the response options did not pose them as such. 

It is worth noting that this study only examined participants' stated categorizations, and it is possible that they may have made other categorizations internally that were not reflected in their responses. However, it is important to recognize that a purely behavioral study such as this cannot fully capture the neurological processes underlying gender perception, which may require more sophisticated techniques such as fMRI and EEEG (Freeman et al 2010, Kloth et al, 2011).

In this study we aggregated responses that did not indicate woman or man. In the multiple response option condition, both “I don’t know” and “Non-binary” were included as a beyond binary categorization. We justified this on the basis that what we are interested in is any categorization beyond the binary. However, these two options are not the same. Furthermore, it is important to note that no matter how a person looks like it is impossible to know their binary or non-binary gender identity (ref). Therefore, if a person aims to be inclusive and not categorize in a binary way, then "I don't know" options are better. 

Based on these findings, we recommend researchers to carefully consider their measurements of  gender categorization. Open text-boxes, forced choice-alternatives and dimensional scales are all viable alternatives. Even researchers who are primarily interested in binary categorizations should consider including beyond-binary alternatives, to avoid perpetuating cisgenderism and to accurately represent the diversity of gender. 


#### Conclusion

In two experiments we tested how different response alternatives affected gender categorizations. Participants were more likely to categorise faces beyond the binary when using a forced-choice paradigm including “non-binary” and “I don’t know” than when using a free text option, or slider scales.  In comparison to self-identification questions were open ended responses are preferred, other categorization might need response options that explicitly reminds participants that not all people identity as woman or man. 

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::




