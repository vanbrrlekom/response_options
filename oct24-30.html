<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Elli van Berlekom">
<meta name="author" content="Stefan Wiens">
<meta name="author" content="Marie Gustafsson Sendén">
<meta name="dcterms.date" content="2024-11-13">
<meta name="description" content="TOWARD INCLUSIVE RESEARCH: THE EFFECT OF RESPONSE OPTIONS ON GENDER CATEGORIZATION OF FACES">

<title>Toward Inclusive Research: The Effect of Response Options on Gender Categorization of Faces</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="nov24_files/libs/clipboard/clipboard.min.js"></script>
<script src="nov24_files/libs/quarto-html/quarto.js"></script>
<script src="nov24_files/libs/quarto-html/popper.min.js"></script>
<script src="nov24_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="nov24_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="nov24_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="nov24_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="nov24_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="nov24_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>


<link rel="stylesheet" href="_extensions/wjschne/apaquarto/apa.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#study-1" id="toc-study-1" class="nav-link active" data-scroll-target="#study-1">Study 1</a></li>
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method">Method</a>
  <ul class="collapse">
  <li><a href="#participants" id="toc-participants" class="nav-link" data-scroll-target="#participants">Participants</a></li>
  <li><a href="#stimuli" id="toc-stimuli" class="nav-link" data-scroll-target="#stimuli">Stimuli</a></li>
  <li><a href="#procedure" id="toc-procedure" class="nav-link" data-scroll-target="#procedure">Procedure</a></li>
  <li><a href="#data-analysis" id="toc-data-analysis" class="nav-link" data-scroll-target="#data-analysis">Data analysis</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  </ul></li>
  <li><a href="#study-2" id="toc-study-2" class="nav-link" data-scroll-target="#study-2">Study 2</a>
  <ul class="collapse">
  <li><a href="#method-1" id="toc-method-1" class="nav-link" data-scroll-target="#method-1">Method</a>
  <ul class="collapse">
  <li><a href="#participants-1" id="toc-participants-1" class="nav-link" data-scroll-target="#participants-1">Participants</a></li>
  <li><a href="#stimuli-1" id="toc-stimuli-1" class="nav-link" data-scroll-target="#stimuli-1">Stimuli</a></li>
  <li><a href="#design-and-procedure" id="toc-design-and-procedure" class="nav-link" data-scroll-target="#design-and-procedure">Design and Procedure</a></li>
  <li><a href="#data-analysis-1" id="toc-data-analysis-1" class="nav-link" data-scroll-target="#data-analysis-1">Data analysis</a></li>
  </ul></li>
  <li><a href="#results-1" id="toc-results-1" class="nav-link" data-scroll-target="#results-1">Results</a></li>
  <li><a href="#discussion-1" id="toc-discussion-1" class="nav-link" data-scroll-target="#discussion-1">Discussion</a></li>
  </ul></li>
  <li><a href="#general-discussion" id="toc-general-discussion" class="nav-link" data-scroll-target="#general-discussion">General Discussion</a>
  <ul class="collapse">
  <li><a href="#limitations-and-future-directions" id="toc-limitations-and-future-directions" class="nav-link" data-scroll-target="#limitations-and-future-directions">Limitations and future directions</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">




<br>

<br>

<section id="title" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Toward Inclusive Research: The Effect of Response Options on Gender Categorization of Faces</h1>
<div class="Author">
<br>

<p>Elli van Berlekom, Stefan Wiens, and Marie Gustafsson Sendén</p>
<p>Psychology Department, Stockholm University</p>
</div>
</section>
<section id="author-note" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Author Note</h1>
<p>Elli van Berlekom <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" width="16" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0002-1949-5600</p>
<p>Stefan Wiens <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" width="16" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0003-4531-4313</p>
<p>Marie Gustafsson Sendén <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" width="16" alt="Orcid ID Logo: A green circle with white letters ID"> http://orcid.org/0000-0002-8393-5316</p>
<p>Correspondence concerning this article should be addressed to Elli van Berlekom, Psychology Department, Stockholm University, Email: elli.vanberlekom@psychology.su.se</p>
</section>
<section id="abstract" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Abstract</h1>
<div class="Abstract">
<p>Psychological research often treats gender as binary and unidimensional, even though this oversimplifies the variability of gender. In two experiments, we studied how alternative response options influence how people perceive the gender of a racially diverse set of morphed faces. Experiment 1 (N = 71) compared one-dimensional and two-dimensional scales for gender categorization. Results indicated that participants consistently used the ends of dimensional scales, resulting in highly categorical perceptions. Experiment 2 (N = 100) compared traditional binary response options with multiple categories and free-text answers. Results indicated that providing more response options than the binary (e.g., “non-binary”, “I don’t know”) made participants perceive gender as more diverse. In contrast, free-text options did not change a binary categorical perception of gender. Despite the opportunity to categorize faces beyond the binary, the predominant categorizations remained as ‘woman’ or ‘man’. We conclude that multiple response options seem to be the best way to increase perceivers’ understanding of gender beyond the binary.</p>
</div>
</section>
<section id="firstheader" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Toward Inclusive Research: The Effect of Response Options on Gender Categorization of Faces</h1>
<p>Many transgender and gender diverse people experience gender as fluid, diffuse, and outside the typical binary of women and men <span class="citation" data-cites="hydeFutureSexGender2019 richardsNonbinaryGenderqueerGenders2016">(<a href="#ref-hydeFutureSexGender2019" role="doc-biblioref">Hyde et al., 2019</a>; <a href="#ref-richardsNonbinaryGenderqueerGenders2016" role="doc-biblioref">Richards et al., 2016a</a>)</span>. Unlike cisgender people - who identify with their assigned gender at birth - transgender people identify with a gender different from their assigned sex at birth <span class="citation" data-cites="levittBeingTransgenderExperience2014">(<a href="#ref-levittBeingTransgenderExperience2014" role="doc-biblioref">Levitt &amp; Ippolito, 2014</a>)</span>. Moreover, many transgender people identify as nonbinary, which can be either an identity in and of itself or an umbrella term for a wide variety of gender identities other than woman or man (e.g., genderqueer, agender, genderfluid) <span class="citation" data-cites="monroNonbinaryGenderqueerOverview2019">(<a href="#ref-monroNonbinaryGenderqueerOverview2019" role="doc-biblioref">Monro, 2019</a>)</span>.</p>
<p>Surveys and questionnaires that collect data on gender have typically used response formats that do not account for this gender diversity, for example by including only the categories woman/female and male/man as response options <span class="citation" data-cites="sapersteinCategoricalGradationalAlternative2021">(<a href="#ref-sapersteinCategoricalGradationalAlternative2021" role="doc-biblioref">Saperstein &amp; Westbrook, 2021</a>)</span>. This is beginning to change; it is increasingly common for studies to include response formats that are sensitive to a wide variety of gender identities, at least when collecting data on participants’ own gender identities <span class="citation" data-cites="hydeFutureSexGender2019">(<a href="#ref-hydeFutureSexGender2019" role="doc-biblioref">Hyde et al., 2019</a>)</span>. Research on gender categorization of others, however, is still dominated by binary response options <span class="citation" data-cites="campanellaCategoricalPerceptionFacial2001 habibiSpontaneousGenderCategorization2012 jungAutomaticityGenderCategorization2019">(e.g., <a href="#ref-campanellaCategoricalPerceptionFacial2001" role="doc-biblioref">Campanella et al., 2001</a>; <a href="#ref-habibiSpontaneousGenderCategorization2012" role="doc-biblioref">Habibi &amp; Khurana, 2012</a>; <a href="#ref-jungAutomaticityGenderCategorization2019" role="doc-biblioref">Jung et al., 2019</a>)</span>. This is potentially problematic; gender categorization research needs to accurately capture the full complexity of gender; moreover, gender categorization is susceptible to influence by short-term methodological factors (Atwood et al., 2024; Thorne et al., 2015). The importance of research on gender diversity and the risk of influencing participants suggests that response formats themselves need to be examined. The aim of this research is therefore to examine two types of response formats that challenge binary gender: multidimensional scales and multiple response options beyond the binary.</p>
<section id="gender-and-gender-categorization" class="level2">
<h2 data-anchor-id="gender-and-gender-categorization">Gender and Gender Categorization</h2>
<p>To understand why response formats matter, it is important to understand the concept of gender itself. Gender is a multifaceted concept with both personal and cultural aspects <span class="citation" data-cites="hydeFutureSexGender2019">(<a href="#ref-hydeFutureSexGender2019" role="doc-biblioref">Hyde et al., 2019</a>)</span>. The personal aspects of gender include a person’s physical body (i.e., sex) and their internal sense of gender (i.e., gender identity). Both sex and gender vary on a spectrum; sex is determined by chromosomes, hormones, and physical morphology, none of which need conform to the typical binary and gender can take an encompass an even broader range of expressions and categories. The cultural aspects of gender include beliefs about which gender categories exist, prescriptions and proscriptions for appropriate behavior for different gender categories, as well as the language available to refer to gender, including gendered pronouns and the names of gender categories (Lonergan &amp; Palomares, 2020).</p>
<p>The personal and cultural aspects of gender are interconnected <span class="citation" data-cites="morgenrothEffectsGenderTrouble2021">(<a href="#ref-morgenrothEffectsGenderTrouble2021" role="doc-biblioref">Morgenroth &amp; Ryan, 2021</a>)</span>; cultural norms often constrain the personal aspects of gender, and, inversely, individuals’ enactments of gender may influence cultural norms over time <span class="citation" data-cites="butler1999">(<a href="#ref-butler1999" role="doc-biblioref">Butler, 1999</a>)</span>. For example, in many cultures that conceive of gender as binary, there are no words or few to express non-binary gender, limiting gender diverse people’s ability to know and express their identity <span class="citation" data-cites="richardsNonbinaryGenderqueerGenders2016a">N. Thorne et al. (<a href="#ref-thorne2023" role="doc-biblioref">2023</a>)</span>. Additionally, the interconnection between the personal and the cultural aspects of gender suggests that inter-individual interactions are important arenas for the emergence and enforcement of gender norms. Treating someone according to the prescribed norms for a specific gender can be impactful, especially for gender diverse people: such actions can be affirming if consistent with that person’s gender identity, negating if not <span class="citation" data-cites="monroNonbinaryGenderqueerOverview2019">Fasoli et al. (<a href="#ref-fasoliWhoYouThink2023" role="doc-biblioref">2023</a>)</span>.</p>
<p>Indeed, many studies suggest that gender categorization is a determining feature of social interactions (Kessler &amp; McKenna, 1978; Liberman et al., 2017) and can activate stereotypes and associations <span class="citation" data-cites="freemanDynamicInteractiveTheory2011">(<a href="#ref-freemanDynamicInteractiveTheory2011" role="doc-biblioref">Freeman &amp; Ambady, 2011</a>)</span>. People base their gender categorizations on others’ external features <span class="citation" data-cites="yamaguchi">Cloutier et al. (<a href="#ref-cloutierInvestigatingEarlyStages2014" role="doc-biblioref">2014</a>)</span>) in a process that is effortless <span class="citation" data-cites="yangHardDisruptCategorization2019">(<a href="#ref-yangHardDisruptCategorization2019" role="doc-biblioref">Yang &amp; Dunham, 2019</a>)</span>, fast <span class="citation" data-cites="tomelleriNatureGenderCategorization2012">(<a href="#ref-tomelleriNatureGenderCategorization2012" role="doc-biblioref">Tomelleri &amp; Castelli, 2012</a>)</span>, and does not require much visual input <span class="citation" data-cites="ref">(<a href="#ref-ref" role="doc-biblioref"><strong>ref?</strong></a>)</span>. Indeed, based on this evidence, some have concluded that gender categorization is an automatic <span class="citation" data-cites="jungAutomaticityGenderCategorization2019">(<a href="#ref-jungAutomaticityGenderCategorization2019" role="doc-biblioref">Jung et al., 2019</a>)</span> and unavoidable feature of social interactions (Fiske, 1998).</p>
<p>Another feature of gender categorization is that responses display a pattern consistent with categorical perception <span class="citation" data-cites="campanellaCategoricalPerceptionFacial2001">(<a href="#ref-campanellaCategoricalPerceptionFacial2001" role="doc-biblioref">Campanella et al., 2001</a>)</span>. For example, when people categorize faces that are morphed to vary on a continuum from feminine to masculine, their categorizations are exaggerated compared to the level of facial gender (Rule et al., 2012; Campanella et al., 2001; Atwood et al., 2024). This means that a 60% female morph may be categorized as a woman by closer to 80% of participants <span class="citation" data-cites="campanellaCategoricalPerceptionFacial2001">(<a href="#ref-campanellaCategoricalPerceptionFacial2001" role="doc-biblioref">Campanella et al., 2001</a>)</span>. Categorical perception patterns have been observed for other domains, such as colors (blue, green) and phonemes (“ga”, “ba”) and suggest that people view stimuli that vary on a continuum as consisting of just a few categories <span class="citation" data-cites="simanovaLinguisticPriorsShape2016">(<a href="#ref-simanovaLinguisticPriorsShape2016" role="doc-biblioref">Simanova et al., 2016</a>)</span>. The observation of categorical perception of gender, therefore, implies that people treat gender as a binary consisting of women and men only.</p>
<p>However, recent work suggests that a binary view of gender is not inevitable. For example, in one study, participants were tasked with keeping track of three people carrying out a conversation <span class="citation" data-cites="gallagher2025">(<a href="#ref-gallagher2025" role="doc-biblioref">Gallagher et al., 2025</a>)</span>. On both implicit and explicit indices of gender categorization, younger participants and participants with personal experience of gender diversity were less likely than other participants to categorize conversationalists by gender [Gallagher et al., 2024]. Additionally, two studies have shown that categorical perception of gender can be disrupted. S. Thorne et al. (<a href="#ref-thorneLeftHemisphereAndrocentric2015">2015</a>) reduced participants’ categorical perception by presenting faces only in the left visual field. These stimuli were processed by the right brain hemisphere, implying that only the left brain hemisphere displayed categorical perception of gender. Moreover, Atwood and colleagues (2024) reduced categorical perception simply by presenting participants with a third response option beyond woman and man. The last study, in particular, suggests the importance of response formats in determining outcomes in categorization tasks.</p>
</section>
<section id="response-formats-and-gender-categorization-beyond-the-binary" class="level2">
<h2 data-anchor-id="response-formats-and-gender-categorization-beyond-the-binary">Response Formats and Gender Categorization Beyond the Binary</h2>
<p>Therefore, it may be troubling that research gender on categorization has largely operationalized gender using response formats that suggest that gender is binary <span class="citation" data-cites="hydeFutureSexGender2019">Lindqvist et al. (<a href="#ref-lindqvistWhatGenderAnyway2020" role="doc-biblioref">2020</a>)</span>. Two examples of such formats are gender categorization expressed as a single dimension (feminine-masculine) and as binary response options(woman/female and man/male). Both response formats imply ideas about gender: a single dimension suggests that femininity and masculinity are mutually exclusive oppositesl and binary response options suggest that those are the only gender categories that exist. It is therefore possible that binary response formats bias participants toward a binary conception of gender.</p>
<p>As gender is diverse, it is important to understand which types of response formats capture gender diversity. One approach is to operationalize femininity and masculinity as independent dimensions. Unlike one-dimensional treatments of gender, a two-dimensional approach does not imply that femininity and masculinity are opposites and that an increase in one necessarily entails a decrease in the other, destabilizing binary notions of gender <span class="citation" data-cites="libenUnderstandingUnderminingDevelopment2017">(<a href="#ref-libenUnderstandingUnderminingDevelopment2017" role="doc-biblioref">Liben &amp; Bigler, 2017</a>)</span>. Two-dimensional approaches have been used to assess gender as a psychological trait (Bem, 1974), as self-idendified categories <span class="citation" data-cites="sapersteinCategoricalGradationalAlternative2021">(<a href="#ref-sapersteinCategoricalGradationalAlternative2021" role="doc-biblioref">Saperstein &amp; Westbrook, 2021</a>)</span> and as categorization of others <span class="citation" data-cites="wittlinFaceMemoryTransgender2018">(<a href="#ref-wittlinFaceMemoryTransgender2018" role="doc-biblioref">Wittlin et al., 2018</a>)</span>. Moreover, at least one study found that femininity and masculinity were independent of each other <span class="citation" data-cites="hesterPerceivedFemininityMasculinity">(<a href="#ref-hesterPerceivedFemininityMasculinity" role="doc-biblioref">Hester et al., n.d.</a>)</span>.</p>
<p>Another approach has been to broaden the response options even further Saperstein and Westbrook (<a href="#ref-sapersteinCategoricalGradationalAlternative2021">2021</a>) by including a range of response options in addition to woman/female and man/male. This approach is increasingly common for assessing participants’ own gender identity (Li et al, 2024., Saperstein et al.) and has been used in a handful of studies of gender categorization of others (van Berlekom et al., 2024; Weissflog et al, 2024). Additionally, Lindqvist et al. (<a href="#ref-lindqvistWhatGenderAnyway2020">2020</a>) suggested that the most sensitive measure for self-categorization would be an open text entry where participants can fill in their gender in an open-ended format. The free text response has the advantage of being completely unconstrained, allowing participants to enter any category.</p>
<p>These approaches may imply that gender is visible from appearance. However, because gender identity is an internal felt sense of self is and not determined by sex, appearance is not always a reliable indicator of gender identity. In real-world contexts, therefore, relying on external attributes to categorize gender can lead to miscategorization. Therefore, in situations where misgendering might occur, it is essential to allow for an ‘I don't know’ option.”</p>
</section>
<section id="the-present-research" class="level2">
<h2 data-anchor-id="the-present-research">The Present Research</h2>
<p>Study 1 investigated the influence of dimensionality on categorical perception of gender. We compared one-dimensional and two-dimensional response formats. Study 2 investigated the influence of response option freedom on the categorization of binary and non-binary gender. We compared binary options, multiple options, and free text entry. Both studies were exploratory and did not include any preregistered hypotheses.</p>
</section>
</section>
<section id="study-1" class="level1">
<h1>Study 1</h1>
</section>
<section id="method" class="level1">
<h1>Method</h1>
<section id="participants" class="level3">
<h3 data-anchor-id="participants">Participants</h3>
<p>Swedish participants (<em>N</em> = 71) <span style="color:green;">completed</span> the study at the Stockholm University campus (<em>M</em><sub>age</sub>= 37.87, <em>SD</em><sub>age</sub> = 14.08, Range = 18 - 73). Participants comprised 33 women, 35 men, and 2 participants who did not indicate gender <span class="citation" data-cites="lindqvistWhatGenderAnyway2020">(self-identified gender was measured using an open-ended text box, following <a href="#ref-lindqvistWhatGenderAnyway2020" role="doc-biblioref">Lindqvist et al., 2020</a>)</span>. Participants were randomly allocated to one of the two response option conditions (<em>N</em><sub>control</sub> = 33, <em>N</em><sub>experimental</sub> = 38). Participants were monetarily compensated for their time (100 SEK). In accordance with the Helsinki Declaration, all participants were informed that participation was voluntary and gave written consent to participate in the study.</p>
</section>
<section id="stimuli" class="level3">
<h3 data-anchor-id="stimuli">Stimuli</h3>
<p>The experiment included Black, Asian, and White faces from the London Face Database <span class="citation" data-cites="debruineFaceResearchLab2017">(<a href="#ref-debruineFaceResearchLab2017" role="doc-biblioref">DeBruine &amp; Jones, 2017</a>)</span> and the Chicago Face Database<span class="citation" data-cites="maChicagoFaceDatabase2015">(<a href="#ref-maChicagoFaceDatabase2015" role="doc-biblioref">Ma et al., 2015</a>)</span> morphed with Webmorph <span class="citation" data-cites="debruineWebMorph2018">(<a href="#ref-debruineWebMorph2018" role="doc-biblioref">DeBruine, 2018</a>)</span>. We selected matched pairs of faces of women and men, ensuring that the women were rated at similar levels of feminine as the men were rated masculine according to the norming data <span class="citation" data-cites="maChicagoFaceDatabase2015">(<a href="#ref-maChicagoFaceDatabase2015" role="doc-biblioref">Ma et al., 2015</a>)</span>. The morphs were made in 7 steps, from completely feminine to completely masculine. <span style="&quot;color:green;">We defined facial gender as the position of the face on the morphing continuum</span>. In other words, a 33% face was slightly masculine, a 50% face was an even mixture of the two faces, and a 100% face consisted only of the woman’s face. Because there were 18 face pairs morphed in 7 steps, the total number of faces was 126.</p>
</section>
<section id="procedure" class="level3">
<h3 data-anchor-id="procedure">Procedure</h3>
<p>Participants completed the experiment on a computer in a quiet room. Each trial consisted of a face accompanied by the question, “How would you gender categorize this person?”. In the <em>one-dimensional</em> condition, participants rated gender based on a single continuum, with the anchors marked <em>woman</em> and <em>man</em>. In the <em>two-dimensional</em> condition, participants rated each face once on a <em>woman</em> continuum (the anchors were marked <em>not woman</em> and <em>woman</em>) and once on a <em>man</em> continuum (anchors were marked <em>not man</em> and <em>man</em>). The separate continua were presented on different trials. For each condition the order of trials was completely randomized in both conditions (see <a href="#fig-exp2-trial" class="quarto-xref" aria-expanded="false">Figure&nbsp;2</a>).</p>
</section>
<section id="data-analysis" class="level3">
<h3 data-anchor-id="data-analysis">Data analysis</h3>
<p>We used R <span class="citation" data-cites="R-base">(Version 4.2.2; <a href="#ref-R-base" role="doc-biblioref">R Core Team, 2022</a>)</span> and the R-packages <em>brms</em> <span class="citation" data-cites="R-brms_a R-brms_b R-brms_c">(Version 2.18.0; <a href="#ref-R-brms_a" role="doc-biblioref">Bürkner, 2017</a>, <a href="#ref-R-brms_b" role="doc-biblioref">2018</a>, <a href="#ref-R-brms_c" role="doc-biblioref">2021</a>)</span>, <em>papaja</em> <span class="citation" data-cites="R-papaja">(Version 0.1.1; <a href="#ref-R-papaja" role="doc-biblioref">Aust &amp; Barth, 2022</a>)</span>, and <em>tidyverse</em> <span class="citation" data-cites="R-tidyverse">(Version 1.3.2; <a href="#ref-R-tidyverse" role="doc-biblioref">Wickham et al., 2019</a>)</span>. We <span style="color:green;">fit Bayesian mixed-effects models to the data</span> to test for patterns of responses consistent with categorical perception. In all models, facial gender (0 to 100 in seven steps) and response options (one-dimensional, two-dimensional) were included as fixed effects. Additionally, all models included varying intercepts for both participants and faces and varying slopes for facial gender. We modeled the predictor facial gender as an unordered factor with seven levels corresponding to each of the seven morphing steps. This allowed us to test for non-linear patterns that would be observed under categorical perception, where changes in facial gender would be expected to have a larger effect on rated gender near the midpoint than at extreme values.</p>
</section>
<section id="results" class="level2">
<h2 data-anchor-id="results">Results</h2>
<p>We examined the relationship between ratings of woman and man in the two-dimensional condition across trials and subjects. These were highly negatively correlated (R = -0.86). Therefore, man ratings in the multiple dimensions were reverse-coded for subsequent analyses. Second, we examined whether participants responded categorically to faces (Research Question 1). Individual-level (thin lines) and group mean (thick lines) responses are visualized in <a href="#fig-desc-two" class="quarto-xref" aria-expanded="false">Figure&nbsp;3</a>. If participants respond according to the morph level, the lines should be a straight diagonal. Instead most participants display a non-linear S-shape, and this was also the pattern of the group means. Note that in the two-dimensional condition, participants rated each face twice.</p>
<p>To further test whether the faces were rated categorically, we calculated the difference between the mean ratings when facial gender was 33% and 67%. If participants respond linearly, this difference should be 34. Instead, in both the one-dimensional condition (<em>M</em><sub>1D</sub> = 59.58, CI<sub>1D</sub> = [53.65, 65.26]) and the two-dimensional condition (<em>M</em><sub>2D</sub> = 58.75, CI<sub>2D</sub> = [52.53, 65.08]) this difference far exceeded 34 and the narrow credible intervals suggest these measures were precisely estimated. We interpret this to mean that participants responded categorically.</p>
<p>Finally, we tested whether the categorical perception was reduced in the two-dimension condition compared to the one-dimension condition (Research Question 2). In other words, we calculated the mean difference between 67% faces and 33% faces and then computed the difference of differences between the two conditions. The results suggested that categorical perception was not reduced by two-dimensional response options (Difference = -0.83, CI = [-5.57, 7.24], BF<sub>01</sub>= 30.47).</p>
</section>
<section id="discussion" class="level2">
<h2 data-anchor-id="discussion">Discussion</h2>
<p>Participants responded categorically when rating faces in terms of gender. Additionally, two-dimensional response options did not reduce this effect. Indeed a binary view of gender was present and participants treated womanhood and manhood as opposites even though the scale would have allowed them to be more flexible. However, this scale only implicitly challenged the binary, as no diverse gender options were present.</p>
</section>
</section>
<section id="study-2" class="level1">
<h1>Study 2</h1>
<p>Study 2 tested a wider range of response options that explicitly challenge the gender binary. These were adapted from common ways to measure participants’ self-categorization of gender <span class="citation" data-cites="lindqvistWhatGenderAnyway2020 sapersteinCategoricalGradationalAlternative2021">(<a href="#ref-lindqvistWhatGenderAnyway2020" role="doc-biblioref">Lindqvist et al., 2020</a>; <a href="#ref-sapersteinCategoricalGradationalAlternative2021" role="doc-biblioref">Saperstein &amp; Westbrook, 2021</a>)</span>.Subjects were randomly assigned to one of three response opotions conditions: 1) binary categories; 2) multiple categories 3) free text.</p>
<section id="method-1" class="level2">
<h2 data-anchor-id="method-1">Method</h2>
<section id="participants-1" class="level3">
<h3 data-anchor-id="participants-1">Participants</h3>
<p>Swedish participants (<em>N</em> = 100) took part in the study at the Stockholm University campus (<em>M</em><sub>age</sub>= 36.89, <em>SD</em><sub>age</sub> = 13.69, Range = 18 - 69). Self-identified gender was measured using an open-ended text box. The final sample comprised 50 women, 47 men, and 3 participants who did not indicate gender. All participants were informed that participation was voluntary and all gave written consent to participate in the study. Participants were randomly allocated to one of the two response option conditions (<em>N</em><sub>binary</sub> = 32, <em>N</em><sub>multiple</sub> = 36, <em>N</em><sub>free_text</sub> = 32). Participants were monetarily compensated for their time (100 sek).</p>
</section>
<section id="stimuli-1" class="level3">
<h3 data-anchor-id="stimuli-1">Stimuli</h3>
<p>The stimuli were identical to those of Study 1.</p>
</section>
<section id="design-and-procedure" class="level3">
<h3 data-anchor-id="design-and-procedure">Design and Procedure</h3>
<p>The experiment used a between-participants design. There were three conditions with different response options: binary categories, free text, and multiple categories (see <a href="#fig-exp1-trial" class="quarto-xref" aria-expanded="false">Figure&nbsp;4</a>). In the <em>binary categories</em> condition, the response options consisted of two categories: woman and man. In the <em>free text</em> condition, the response options consisted of an open text box. In the <em>multiple categories</em> condition, the response options consisted of four categories: woman, man, other, and I don’t know.</p>
<p>Participants completed the experiment on a computer in a quiet room. Each trial consisted of a face accompanied by the question, “How would you gender categorize this person?” Participants categorized 126 faces according to the response options in their condition.</p>
<p>The outcome was responses to the categorization task. For analysis purposes,two new variables were created.</p>
<p><em>Other categorizations</em> represented the trials where participants categorized faces as “other”. This was computed by dichotomizing the variable so other was coded as 1 and all other responses were coded as 0. In the free text condition, participants’ responses were manually coded so that other and non-binary were counted as other.</p>
<p><em>I don’t know</em> responses represented trials where participants did not categorize any gender category. I don’t know was coded as 1 and all other responses were coded as 0. In the free text condition, participants’ responses were manually coded so that variations of unsure were counted as I don’t know.</p>
</section>
<section id="data-analysis-1" class="level3">
<h3 data-anchor-id="data-analysis-1">Data analysis</h3>
<p>Bayesian linear mixed effect models were fit to the data in study 2. These models were the same as in Study 1 apart from the outcome, which was binomial and accordingly had to be modeled as a binomial distribution.’</p>
</section>
</section>
<section id="results-1" class="level2">
<h2 data-anchor-id="results-1">Results</h2>
<p><a href="#fig-desc-1" class="quarto-xref" aria-expanded="false">Figure&nbsp;5</a> illustrates the proportion of faces (y-axis) categorized according to the different conditions (different colors) at each level of facial gender (x-axis) across the three experimental conditions (separate plots). A visual inspection of Figure <a href="#fig-desc-1" class="quarto-xref" aria-expanded="false">Figure&nbsp;5</a> suggests that most faces were categorized as women or men. However, participants did categorize faces outside of this binary in the multiple categories condition, as <a href="#fig-desc-1" class="quarto-xref" aria-expanded="false">Figure&nbsp;5</a> shows, and most such categorizations were made in response to androgynous faces.</p>
<p><a href="#fig-desc-1" class="quarto-xref" aria-expanded="false">Figure&nbsp;5</a>, however, only illustrates the total number of categorizations across all participants. This obscures the fact that some participants made many categorizations beyond the binary and some made few or none at all. <a href="#fig-desc-nbo" class="quarto-xref" aria-expanded="false">Figure&nbsp;6</a> illustrates how many categorizations beyond the binary participants made. Each bar represents how many participants (y-axis) made a certain number of categorizations (x-axis). The different colors denote the different categorizations. Participants who only categorized faces as women or men are not represented in the figure. In the Free Text condition, only two participants made any other categorization than woman and man, whereas more than half did so in the Multiple Categories condition (see <a href="#fig-desc-nbo" class="quarto-xref" aria-expanded="false">Figure&nbsp;6</a> ).The Bayesian mixed effects model suggested that participants made more categorizations beyond the binary in the multiple categories condition compared to the free text condition (OR = 5.56, CI =[1.1, 27.97], BF<sub>10</sub>= 4.55).</p>
<p>Inspection of <a href="#fig-desc-1" class="quarto-xref" aria-expanded="false">Figure&nbsp;5</a> suggests that participants in multiple categories condition made fewer man categorizations than participants in the other two conditions. We tested this by examining only responses of woman or man. <a href="#fig-wtf" class="quarto-xref" aria-expanded="false">Figure&nbsp;8</a> illustrates proportions of responses of women and men. Each dot represents a single participant, and the position of the dots on the y axis shows the proportion of faces that each participant categorized as man; the boxplots show median and interquartile range proportion of faces categorized as men. Overall rates of binary categorizations were similar across the three conditions (see <a href="#fig-wtf" class="quarto-xref" aria-expanded="false">Figure&nbsp;8</a>).</p>
<p>We treated the binary categories condition as the control against which the other two conditions were compared. The results suggested that the proportion of faces categorized as women was similar in the Multiple Categories and Binary Categories conditions (OR = 0.68, CI =[0.4, 1.17], BF<sub>01</sub>= 5.98). The results suggested that the proportion of faces categorized as women was the same in the Free text and Binary Categories condition (OR = 1.03, CI =[0.6, 1.78], BF<sub>01</sub>= 15.27). In sum, neither the free text nor the multiple categories condition changed the pattern of categorization of women and men compared to the binary categories condition.</p>
</section>
<section id="discussion-1" class="level2">
<h2 data-anchor-id="discussion-1">Discussion</h2>
<p>In Experiment 2, we tested how free text options and multiple categories affected participants’ responses beyond the binary. Some participants made some categorizations beyond the binary in the multiple categories condition, but virtually none did so in the free text condition. Furthermore, additional response options reduced the absolute number of faces categorized as women and men (as participants selected some of the other options) but did systematically reduce categorizations of men more than women or vice versa.</p>
</section>
</section>
<section id="general-discussion" class="level1">
<h1>General Discussion</h1>
<p>Across two experiments, we tested how different response options influenced gender categorization. In Study 1, we compared two-dimensional scales with one-dimensional controls. We found that participants responded categorically, and this was the case in both the control condition and the two-dimensional condition. In Study 2, we compared free text and multiple categories. We found that only multiple categories elicited beyond-binary responses. Compared to binary control, neither changed the pattern of categorizations of women and men.</p>
<p>The results from Study 1 are consistent with previous work on categorical perception of gender in faces <span class="citation" data-cites="campanellaCategoricalPerceptionFacial2001 campanellaCategoricalPerceptionUnfamiliar2003">(<a href="#ref-campanellaCategoricalPerceptionFacial2001" role="doc-biblioref">Campanella et al., 2001</a>, <a href="#ref-campanellaCategoricalPerceptionUnfamiliar2003" role="doc-biblioref">2003</a>)</span>. Participants exhibited a categorical pattern of responses where ratings of gender were more extreme than the facial gender. This implies that participants had a conception of gender as consisting of two distinct categories. Furthermore, the two-dimensional ratings did not reduce the strength of the categorical effect. This suggests that, at least in the present sample, two-dimensional response options were not enough to reduce the binary gender norms.</p>
<p>This differs slightly from the results of Bem (<a href="#ref-bemMeasurmentPsychologicalAndrogyny1974">1974</a>), who found that measuring gender as two separate scales led participants to treat gender as less binary. Moreover, where she found that masculinity and femininity were largely unrelated, we found that ratings of woman and man were strongly correlated. This is probably accounted for by the differences in outcome measures in Bem (<a href="#ref-bemMeasurmentPsychologicalAndrogyny1974">1974</a>) and in our study. Bem (<a href="#ref-bemMeasurmentPsychologicalAndrogyny1974">1974</a>) measured gender as a psychological trait in the self, whereas we measured gender as a judgment of the gender identity of others. The latter outcome is not only determined by the response options, but also by the physical features of the faces. In other words, judging the faces of others is a different task from judging one’s own characteristics, and one of the primary differences is the increase in external stimuli and influences.</p>
<p>The finding from Study 2 that participants use non-binary response options is consistent with the work of Saperstein and Westbrook (<a href="#ref-sapersteinCategoricalGradationalAlternative2021">2021</a>) and Lindqvist et al. (<a href="#ref-lindqvistWhatGenderAnyway2020">2020</a>), which has shown that including flexible response options allows participants to better express themselves. A recommendation from that literature is that open text boxes afford participants the greatest flexibility in their responses. In our study that flexibility was rarely used when the response options consisted of a free text. This likely reflects the difference between transgender and gender-diverse participants categorizing their own gender and cisgender participants categorizing others.</p>
<p>A probable explanation for the difference between free text and multiple categories in Study 2 is that the multiple categories served as a visual reminder of non-binary identity. Researchers interested in the categorization of non-binary identity should be aware that these may not spring to mind unless participants are explicitly reminded of them.</p>
<p>Neither free text nor multiple categories influenced the categorizations of women and men. This suggests that such inclusive response options can be suitable for investigating the categorization of women and men without skewing the results or introducing noise. This is a positive finding for researchers who are primarily interested in such categorizations but do not want to contribute to the marginalization of trans and non-binary individuals.</p>
<p>Overall, we recommend researchers include non-binary response options in gender categorization studies. Multiple dimensions, free text, and multiple categories and continua are all viable alternatives. If the primary research question is to investigate non-binary categorization, then multiple categories are most suitable. However, if the goal is to measure the categorization of women and men, free text or multiple categories may be equally suitable.</p>
<section id="limitations-and-future-directions" class="level2">
<h2 data-anchor-id="limitations-and-future-directions">Limitations and future directions</h2>
<p>One limitation of this study is the sample size. The <em>N</em>s in each condition are below many of the conventional recommendations in social psychology. However, these recommendations are typically made based on the assumption of a single trial per participant. In contrast, each participant completed 126 trials in our experiment. This allows for precise detailing of the within-participant processes. As such, the present study resembles psychophysical experiments, which also feature few participants carrying out many trials. Power is often portrayed as a function of sample size, and this is true, the number of trials is also a factor in power <span class="citation" data-cites="juddExperimentsMoreOne2017">(<a href="#ref-juddExperimentsMoreOne2017" role="doc-biblioref">Judd et al., 2017</a>)</span>. Indeed, the overall analyses included more than 8000 data points in each experiment, and the final estimates were measured with a high degree of precision. That said, we note that the generalizability of the experiment is somewhat reduced.</p>
<p>Another limitation of this study is that it does not account for the influence of markers of gender other than faces. Such markers include hair, clothes, and makeup and Transgender and gender diverse often use such markers to signal their gender to others. Moreover, the faces used here were not “realistic” in that they did not realistically depict gender diversity as it is often displayed in the real world. In that sense, it is possible that we underestimate the rates of people responding with one of the options beyond the binary.</p>
</section>
<section id="conclusion" class="level2">
<h2 data-anchor-id="conclusion">Conclusion</h2>
<p>In two studies, we tested how different response alternatives affected gender categorizations. In Study 1, participants responded categorically to the faces, both when rating gender using one-dimensional and two-dimensional scales. This suggests that participants generally had a binary conception of gender, which was not influenced by response options. In Study 2, participants were more likely to categorize faces beyond the binary when using multiple categories, including non-binary and I don’t know than when using a free text option. In comparison to self-identification questions, where open-ended responses are seen as the most inclusive alternative <span class="citation" data-cites="lindqvistWhatGenderAnyway2020">(<a href="#ref-lindqvistWhatGenderAnyway2020" role="doc-biblioref">Lindqvist et al., 2020</a>)</span>, the categorization of others benefits from response options that explicitly remind participants that not all people identify as women or men.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-custom-style="Bibliography" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-R-papaja" class="csl-entry" role="listitem">
Aust, F., &amp; Barth, M. (2022). <em><span class="nocase">papaja</span>: <span>Prepare</span> reproducible <span>APA</span> journal articles with <span>R Markdown</span></em>. <a href="https://github.com/crsh/papaja">https://github.com/crsh/papaja</a>
</div>
<div id="ref-bemMeasurmentPsychologicalAndrogyny1974" class="csl-entry" role="listitem">
Bem, S. L. (1974). The measurment of psychological androgyny. <em>Journal of Consulting and Clinical Psychology</em>, <em>42</em>(2), 155.
</div>
<div id="ref-R-brms_a" class="csl-entry" role="listitem">
Bürkner, P.-C. (2017). <span class="nocase">brms</span>: An <span>R</span> package for <span>Bayesian</span> multilevel models using <span>Stan</span>. <em>Journal of Statistical Software</em>, <em>80</em>(1), 1–28. <a href="https://doi.org/10.18637/jss.v080.i01">https://doi.org/10.18637/jss.v080.i01</a>
</div>
<div id="ref-R-brms_b" class="csl-entry" role="listitem">
Bürkner, P.-C. (2018). Advanced <span>Bayesian</span> multilevel modeling with the <span>R</span> package <span class="nocase">brms</span>. <em>The R Journal</em>, <em>10</em>(1), 395–411. <a href="https://doi.org/10.32614/RJ-2018-017">https://doi.org/10.32614/RJ-2018-017</a>
</div>
<div id="ref-R-brms_c" class="csl-entry" role="listitem">
Bürkner, P.-C. (2021). Bayesian item response modeling in <span>R</span> with <span class="nocase">brms</span> and <span>Stan</span>. <em>Journal of Statistical Software</em>, <em>100</em>(5), 1–54. <a href="https://doi.org/10.18637/jss.v100.i05">https://doi.org/10.18637/jss.v100.i05</a>
</div>
<div id="ref-butler1999" class="csl-entry" role="listitem">
Butler, J. (1999). <em>Gender trouble: feminism and the subversion of identity</em>. Routledge.
</div>
<div id="ref-campanellaCategoricalPerceptionFacial2001" class="csl-entry" role="listitem">
Campanella, S., Chrysochoos, A., &amp; Bruyer, R. (2001). Categorical perception of facial gender information: <span>Behavioural</span> evidence and the face-space metaphor. <em>Visual Cognition</em>, <em>8</em>(2), 237–262. <a href="https://doi.org/10.1080/13506280042000072">https://doi.org/10.1080/13506280042000072</a>
</div>
<div id="ref-campanellaCategoricalPerceptionUnfamiliar2003" class="csl-entry" role="listitem">
Campanella, S., Hanoteau, C., Seron, X., Joassin, F., &amp; Bruyer, R. (2003). Categorical perception of unfamiliar facial identities, the face-space metaphor, and the morphing technique. <em>Visual Cognition</em>, <em>10</em>(2), 129–156. <a href="https://doi.org/10.1080/713756676">https://doi.org/10.1080/713756676</a>
</div>
<div id="ref-cloutierInvestigatingEarlyStages2014" class="csl-entry" role="listitem">
Cloutier, J., Freeman, J. B., &amp; Ambady, N. (2014). Investigating the early stages of person perception: <span>The</span> asymmetry of social categorization by sex vs. age. <em>PLoS ONE</em>, <em>9</em>(1), e84677. <a href="https://doi.org/10.1371/journal.pone.0084677">https://doi.org/10.1371/journal.pone.0084677</a>
</div>
<div id="ref-debruineWebMorph2018" class="csl-entry" role="listitem">
DeBruine, L. (2018). <span>WebMorph</span>. In <em>WebMorph</em>. https://webmorph.org/.
</div>
<div id="ref-debruineFaceResearchLab2017" class="csl-entry" role="listitem">
DeBruine, L., &amp; Jones, B. C. (2017). Face <span>Research Lab London Set</span>. <em>Figshare</em>. <a href="https://doi.org/10.6084/m9.figshare.5047666">https://doi.org/10.6084/m9.figshare.5047666</a>
</div>
<div id="ref-fasoliWhoYouThink2023" class="csl-entry" role="listitem">
Fasoli, F., Divine, I., &amp; Hopkins-Doyle, A. (2023). Who do you think they are? <span>An</span> exploratory analysis of face-based impressions formed by cisgender, transgender and gender nonbinary individuals. <em>Routledge Open Research</em>, <em>2</em>, 38. <a href="https://doi.org/10.12688/routledgeopenres.17991.1">https://doi.org/10.12688/routledgeopenres.17991.1</a>
</div>
<div id="ref-freemanDynamicInteractiveTheory2011" class="csl-entry" role="listitem">
Freeman, J. B., &amp; Ambady, N. (2011). A dynamic interactive theory of person construal. <em>Psychological Review</em>, <em>118</em>(2), 247–279. <a href="https://doi.org/10.1037/a0022327">https://doi.org/10.1037/a0022327</a>
</div>
<div id="ref-gallagher2025" class="csl-entry" role="listitem">
Gallagher, N. M., Foster-Hanson, E., &amp; Olson, K. R. (2025). Gender categorization and memory in transgender and cisgender people. <em>Journal of Experimental Social Psychology</em>, <em>116</em>, 104691. <a href="https://doi.org/10.1016/j.jesp.2024.104691">https://doi.org/10.1016/j.jesp.2024.104691</a>
</div>
<div id="ref-habibiSpontaneousGenderCategorization2012" class="csl-entry" role="listitem">
Habibi, R., &amp; Khurana, B. (2012). Spontaneous <span>Gender Categorization</span> in <span>Masking</span> and <span>Priming Studies</span>: <span>Key</span> for <span>Distinguishing Jane</span> from <span>John Doe</span> but <span>Not Madonna</span> from <span>Sinatra</span>. <em>PLoS ONE</em>, <em>7</em>(2), e32377. <a href="https://doi.org/10.1371/journal.pone.0032377">https://doi.org/10.1371/journal.pone.0032377</a>
</div>
<div id="ref-hesterPerceivedFemininityMasculinity" class="csl-entry" role="listitem">
Hester, N., Jones, B. C., &amp; Hehman, E. (n.d.). <em>Perceived <span>Femininity</span> and <span>Masculinity Contribute Independently</span> to <span>Facial Impressions</span></em>. 56.
</div>
<div id="ref-hydeFutureSexGender2019" class="csl-entry" role="listitem">
Hyde, J. S., Bigler, R. S., Joel, D., Tate, C. C., &amp; van Anders, S. M. (2019). The future of sex and gender in psychology: <span>Five</span> challenges to the gender binary. <em>American Psychologist</em>. <a href="https://doi.org/10.1037/amp0000307">https://doi.org/10.1037/amp0000307</a>
</div>
<div id="ref-juddExperimentsMoreOne2017" class="csl-entry" role="listitem">
Judd, C. M., Westfall, J., &amp; Kenny, D. A. (2017). Experiments with <span>More Than One Random Factor</span>: <span>Designs</span>, <span>Analytic Models</span>, and <span>Statistical Power</span>. <em>Annual Review of Psychology</em>, <em>68</em>(1), 601–625. <a href="https://doi.org/10.1146/annurev-psych-122414-033702">https://doi.org/10.1146/annurev-psych-122414-033702</a>
</div>
<div id="ref-jungAutomaticityGenderCategorization2019" class="csl-entry" role="listitem">
Jung, K. H., White, K. R. G., &amp; Powanda, S. J. (2019). Automaticity of gender categorization: <span>A</span> test of the efficiency feature. <em>Social Cognition</em>, <em>37</em>(2), 122–144. <a href="https://doi.org/10.1521/soco.2019.37.2.122">https://doi.org/10.1521/soco.2019.37.2.122</a>
</div>
<div id="ref-levittBeingTransgenderExperience2014" class="csl-entry" role="listitem">
Levitt, H. M., &amp; Ippolito, M. R. (2014). Being transgender: <span>The</span> experience of transgender identity development. <em>Journal of Homosexuality</em>, <em>61</em>(12), 1727–1758. <a href="https://doi.org/10.1080/00918369.2014.951262">https://doi.org/10.1080/00918369.2014.951262</a>
</div>
<div id="ref-libenUnderstandingUnderminingDevelopment2017" class="csl-entry" role="listitem">
Liben, L. S., &amp; Bigler, R. S. (2017). Understanding and <span>Undermining</span> the <span>Development</span> of <span>Gender Dichotomies</span>: <span>The Legacy</span> of <span>Sandra Lipsitz Bem</span>. <em>Sex Roles</em>, <em>76</em>(9-10), 544–555. <a href="https://doi.org/10.1007/s11199-015-0519-4">https://doi.org/10.1007/s11199-015-0519-4</a>
</div>
<div id="ref-lindqvistWhatGenderAnyway2020" class="csl-entry" role="listitem">
Lindqvist, A., Sendén, M. G., &amp; Renström, E. A. (2020). What is gender, anyway: A review of the options for operationalising gender. <em>Psychology &amp; Sexuality</em>, 1–13. <a href="https://doi.org/10.1080/19419899.2020.1729844">https://doi.org/10.1080/19419899.2020.1729844</a>
</div>
<div id="ref-maChicagoFaceDatabase2015" class="csl-entry" role="listitem">
Ma, D. S., Correll, J., &amp; Wittenbrink, B. (2015). The <span>Chicago</span> face database: <span>A</span> free stimulus set of faces and norming data. <em>Behavior Research Methods</em>, <em>47</em>(4), 1122–1135. <a href="https://doi.org/10.3758/s13428-014-0532-5">https://doi.org/10.3758/s13428-014-0532-5</a>
</div>
<div id="ref-monroNonbinaryGenderqueerOverview2019" class="csl-entry" role="listitem">
Monro, S. (2019). Non-binary and genderqueer: <span>An</span> overview of the field. <em>International Journal of Transgenderism</em>, <em>20</em>(2-3), 126–131. <a href="https://doi.org/10.1080/15532739.2018.1538841">https://doi.org/10.1080/15532739.2018.1538841</a>
</div>
<div id="ref-morgenrothEffectsGenderTrouble2021" class="csl-entry" role="listitem">
Morgenroth, T., &amp; Ryan, M. K. (2021). The effects of gender trouble: <span>An</span> integrative theoretical framework of the perpetuation and disruption of the gender/sex binary. <em>Perspectives on Psychological Science</em>, <em>16</em>(6), 1113–1142. <a href="https://doi.org/10.1177/1745691620902442">https://doi.org/10.1177/1745691620902442</a>
</div>
<div id="ref-R-base" class="csl-entry" role="listitem">
R Core Team. (2022). <em>R: A language and environment for statistical computing</em>. R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>
</div>
<div id="ref-richardsNonbinaryGenderqueerGenders2016" class="csl-entry" role="listitem">
Richards, C., Bouman, W. P., Seal, L., Barker, M. J., Nieder, T. O., &amp; T’Sjoen, G. (2016a). Non-binary or genderqueer genders. <em>Int Rev Psychiatry .</em>, <em>28(1)</em>, 95–102.
</div>
<div id="ref-richardsNonbinaryGenderqueerGenders2016a" class="csl-entry" role="listitem">
Richards, C., Bouman, W. P., Seal, L., Barker, M. J., Nieder, T. O., &amp; T’Sjoen, G. (2016b). Non-binary or genderqueer genders. <em>International Review of Psychiatry</em>, <em>28</em>(1), 95–102. <a href="https://doi.org/10.3109/09540261.2015.1106446">https://doi.org/10.3109/09540261.2015.1106446</a>
</div>
<div id="ref-sapersteinCategoricalGradationalAlternative2021" class="csl-entry" role="listitem">
Saperstein, A., &amp; Westbrook, L. (2021). Categorical and gradational: Alternative survey measures of sex and gender. <em>European Journal of Politics and Gender</em>, <em>4</em>(1), 11–30. <a href="https://doi.org/10.1332/251510820X15995647280686">https://doi.org/10.1332/251510820X15995647280686</a>
</div>
<div id="ref-simanovaLinguisticPriorsShape2016" class="csl-entry" role="listitem">
Simanova, I., Francken, J. C., de Lange, F. P., &amp; Bekkering, H. (2016). Linguistic priors shape categorical perception. <em>Language, Cognition and Neuroscience</em>, <em>31</em>(1), 159–165. <a href="https://doi.org/10.1080/23273798.2015.1072638">https://doi.org/10.1080/23273798.2015.1072638</a>
</div>
<div id="ref-thorne2023" class="csl-entry" role="listitem">
Thorne, N., Aldridge, Z., Yip, A. K.-T., Bouman, W. P., Marshall, E., &amp; Arcelus, J. (2023). <span>‘</span>I Didn<span>’</span>t Have the Language Then<span>’</span><span></span>A Qualitative Examination of Terminology in the Development of Non-Binary Identities. <em>Healthcare</em>, <em>11</em>(7), 960. <a href="https://doi.org/10.3390/healthcare11070960">https://doi.org/10.3390/healthcare11070960</a>
</div>
<div id="ref-thorneLeftHemisphereAndrocentric2015" class="csl-entry" role="listitem">
Thorne, S., Hegarty, P., &amp; Catmur, C. (2015). Is the left hemisphere androcentric? <span>Evidence</span> of the learned categorical perception of gender. <em>Laterality: Asymmetries of Body, Brain and Cognition</em>, <em>20</em>(5), 571–584. <a href="https://doi.org/10.1080/1357650X.2015.1016529">https://doi.org/10.1080/1357650X.2015.1016529</a>
</div>
<div id="ref-tomelleriNatureGenderCategorization2012" class="csl-entry" role="listitem">
Tomelleri, S., &amp; Castelli, L. (2012). On the <span>Nature</span> of <span>Gender Categorization</span>: <span>Pervasive</span> but <span>Flexible</span>. <em>Social Psychology</em>, <em>43</em>(1), 14–27. <a href="https://doi.org/10.1027/1864-9335/a000076">https://doi.org/10.1027/1864-9335/a000076</a>
</div>
<div id="ref-R-tidyverse" class="csl-entry" role="listitem">
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the <span class="nocase">tidyverse</span>. <em>Journal of Open Source Software</em>, <em>4</em>(43), 1686. <a href="https://doi.org/10.21105/joss.01686">https://doi.org/10.21105/joss.01686</a>
</div>
<div id="ref-wittlinFaceMemoryTransgender2018" class="csl-entry" role="listitem">
Wittlin, N. M., Dovidio, J. F., LaFrance, M., &amp; Burke, S. E. (2018). About face: <span>Memory</span> for transgender versus cisgender targets’ facial appearance. <em>Journal of Experimental Social Psychology</em>, <em>78</em>, 77–92. <a href="https://doi.org/10.1016/j.jesp.2018.04.009">https://doi.org/10.1016/j.jesp.2018.04.009</a>
</div>
<div id="ref-yamaguchi" class="csl-entry" role="listitem">
Yamaguchi, M. K., Hirukawa, T., &amp; Kanazawa, S. (n.d.). <em>Judgment of Gender through Facial Parts</em>. 13.
</div>
<div id="ref-yangHardDisruptCategorization2019" class="csl-entry" role="listitem">
Yang, X., &amp; Dunham, Y. (2019). Hard to disrupt: <span>Categorization</span> and enumeration by gender and race from mixed displays. <em>Journal of Experimental Social Psychology</em>, <em>85</em>, 103893. <a href="https://doi.org/10.1016/j.jesp.2019.103893">https://doi.org/10.1016/j.jesp.2019.103893</a>
</div>
</div>
<div id="fig-stimuli" class="quarto-figure quarto-figure-center quarto-float FigureWithoutNote" prefix="" data-fignum="1" data-custom-style="FigureWithoutNote">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-stimuli-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;1</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Example of a seven-step morphing spectrum</p>
</div>
</figcaption>
<div aria-describedby="fig-stimuli-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pix/stimuli.jpeg" class="img-fluid figure-img">
</div>
</figure>
</div>
<div id="fig-exp2-trial" class="quarto-figure quarto-figure-center quarto-float FigureWithoutNote" prefix="" data-fignum="2" data-custom-style="FigureWithoutNote">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-exp2-trial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;2</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Sample trial from each of the three conditions</p>
</div>
</figcaption>
<div aria-describedby="fig-exp2-trial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pix/exp2.png" class="img-fluid figure-img">
</div>
</figure>
</div>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'gridExtra'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:dplyr':

    combine</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-desc-two" class="quarto-figure quarto-figure-center quarto-float" prefix="" data-fignum="3">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-desc-two-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;3</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Participant level and mean ratings of faces in One-dimensiona and two-dimensional conditions</p>
</div>
</figcaption>
<div aria-describedby="fig-desc-two-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="nov24_files/figure-html/fig-desc-two-1.svg" class="img-fluid figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div id="fig-exp1-trial" class="quarto-figure quarto-figure-center quarto-float FigureWithoutNote" prefix="" data-fignum="4" data-custom-style="FigureWithoutNote">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-exp1-trial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;4</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Sample trial from each of the three conditions</p>
</div>
</figcaption>
<div aria-describedby="fig-exp1-trial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="pix/exp1.png" class="img-fluid figure-img">
</div>
</figure>
</div>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-desc-1" class="quarto-figure quarto-figure-center quarto-float" prefix="" data-fignum="5">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-desc-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;5</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Gender Categorizations by Participants</p>
</div>
</figcaption>
<div aria-describedby="fig-desc-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="nov24_files/figure-html/fig-desc-1-1.svg" class="img-fluid figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-desc-nbo" class="quarto-figure quarto-figure-center quarto-float" prefix="" data-fignum="6">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-desc-nbo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;6</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Responses of other and I don't know across the multiple categories and free text condiitons</p>
</div>
</figcaption>
<div aria-describedby="fig-desc-nbo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="nov24_files/figure-html/fig-desc-nbo-1.svg" class="img-fluid figure-img" width="576">
</div>
</figure>
</div>
</div>
</div>
<div class="cell FigureWithoutNote" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-cond" class="quarto-figure quarto-figure-center quarto-float" prefix="" data-fignum="7">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-cond-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;7</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Alternative version of the previous figures</p>
</div>
</figcaption>
<div aria-describedby="fig-cond-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="nov24_files/figure-html/fig-cond-1.svg" class="img-fluid figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div class="cell FigureWithoutNote" data-fig.heigh="6" data-fig.note="Notches correspond to the Standard Error of the median" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-wtf" class="quarto-figure quarto-figure-center quarto-float" prefix="" data-fignum="8">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-wtf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;8</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Participant Proportions for Categorizing Faces as Men Across Experimental Conditions</p>
</div>
</figcaption>
<div aria-describedby="fig-wtf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="nov24_files/figure-html/fig-wtf-1.svg" class="img-fluid figure-img" width="1152">
</div>
</figure>
</div>
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>